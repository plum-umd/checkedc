Review #70A
Overall merit
3. 
Major revision

Reviewer expertise
3. 
Knowledgeable

Paper summary
Checked C is a variant of C that aims to be memory-safe. Checked C has been in development for several years now, but the existing formalization did not cover a lot of ground. This paper presents a new formalization of Checked C that covers a larger subset of Checked C, compared to the earlier formalization. Notably, the new formalization now accounts for null-terminated arrays, i.e. arrays whose statically known length can be extended via runtime checks.

The contributions are:
- a mechanized formalization of Checked C, with formal proofs of:
  * type soundness
  * blame, i.e. if the program is stuck, the error can be traced to an unsafe
    block that opted out of the Checked C discipline.
- a formalization of the bounds-checking compilation scheme as implemented by
  CheckedC; this second formalization does not come with a compilation
  correctness theorem (i.e. a formal proof that the compiled program simulates
  original fat pointer semantics), but rather relies on the fact that the
  compilation scheme is expressed in PLT-Redex to enable randomized testing,
  which increases confidence that the formalized compilation scheme is correct
- a randomized testing scheme to ascertain that the formalized compilation
  scheme for bounds-check is faithful to what the CheckedC implementation is
  doing.

Strengths
- I like the combination and variety of techniques -- full formal verification
  is not always an option, and exploring alternative techniques (PLT-Redex +
  exhaustive testing) is interesting.
- CheckedC seems to be gaining some traction (as exemplified by FreeBSD) --
  highly relevant problem.
- Serious proof development in Coq for the first part
- Novel formalization for null-terminated arrays and the dynamic semantics of
  run-time testing. In a sense, this encodes a set of automated proof rules for
  a highly dependent type system, establishing a set of supported idioms for
  doing dependent pattern-matching.
- Found actual bugs in checkedc implementation.

Weaknesses
- I had a really hard time understanding precisely the "formalized" compilation
  from CoreChkC to CoreC. Specifically: is CoreC intended to model LLVM-IR, or
  is it a subset of C? Is CheckedC compiling to C or is it a new frontend like
  `clang`? See remark about undefined behavior (UB) below which got me even more
  confused.
- Similar confusion on my part for section V, i.e. ascertaining that the
  compilation scheme from CoreChkC to CoreC is faithful to the actual
  compilation scheme of the checkedc-clang compiler to ...? I feel like the
  paper is missing an actual description of what happens in the compiler to
  allow us to connect the dots and understand how the formalization illuminates
  the implementation.
- Many mistakes in code samples, paper feels like it could've used a few more
  rounds of polishing and copy-editing. Familiarity with CheckedC seemed
  required at times.
- The paper doesn't seem upfront about what is *shown* (theorem in Coq) and what
  is *tested* (via PLT-Redex), and thus remains a conjecture.
- Missing discussion of why Coq vs. PLT-Redex, effort involved, any plans to
  formally prove compilation from CoreChkC to CoreC, any hopes of integrating
  that in the official implementation, etc.

Comments for author
### Pressing questions:

- Fig 9: if this is actual C code, then your null-check at line 6 will be
  eliminated by the compiler. At line 3, you performed a pointer addition, which
  is only defined when `p` is non-null. So, either `p` is non-null, and the
  NULL-check can be eliminated; or, `p` is NULL, but line 3 was undefined
  behavior, meaning the compiler is allowed to do anything, notably eliminate
  the NULL-check. This is where I am super confused, and either:
  - CoreC is not really the C language, and has different semantics...? but is
    this well-defined in the context of LLVM?
  - there is a problem that was not caught by the PLT-Redex-based testing.
- III.C: text says `u < c` but this seems to contradict the paragraph right
  after, because if `m = u` then `u <= c` and it seems like T-DefArr *does*
  allow me to dereference a checked pointer in unchecked mode...? Or am I
  missing something?

### Rest:

- Beginning of page 2: "we show", then "give confidence". I am left confused as
  to whether the CoreChkC --> CoreC compilation is shown correct (theorem with
  proof), or carefully debugged and tested in plt-redex (which is fine too, I
  just find the choice of terminology confusing)
- End of I.: a diagram would be helpful understanding how III, IV and V relate
  to each other and to clang-checkedc
- II.B.: LaTeX issue, your inline monospaced code snippets generate a space on
  the following line (i.e. "a program may read from (but not write to)" is not
  aligned with the left boundary of the paragraph).
- III.A.: the "dynamic cast" terminology may be briefly confused for C++'s
  RTTI-based dynamic cast feature
- III.B.: repeat that the stack is immutable at this point?
- III.B.: this raises a fair amount of questions regarding the treatment of the
  NULL pointer at this stage of the paper... is it modeled as 0, as returned by
  `malloc`? are dynamic checks inserted by CheckedC to guarantee that no NULL
  pointer is dereferenced?
- III.B: "according *to the* compulation relation" (missing "to the"?)
- III.B: the special case raises questions, e.g. why is this syntax-driven and
  not type-driven?
- III.B: it was hard to tell which cases were stuck states, or could reduce
  owing to a rule that was not shown
- Fig 4: can the rules be presented in the same order they were introduced in
  the paper?
- Fig 4, S-FUN: $\vec\tau_a$ seems unused; why?

The example from Fig 5 was extremely hard to follow.
- The caption claims to describe an implementation of strncat, without
  specifying what `strncat` is intended to do. On my machine:

```
     char *
     strncat(char *restrict s1, const char *restrict s2, size_t n);

     The strncat() function appends a copy of the null-terminated string s2 to
     the end of the null-terminated string s1, then adds a terminating ‘\0’.
     The string s1 must have sufficient space to hold the result.

     The strncat() function appends not more than n characters from s2, and then adds a terminating ‘\0’. ```

  but, based on the code sample, your function appears to be a safe
  implementation of `strcat` where the parameter `n` (same name as in `man
  strncat`) actually is the length of the first argument `a`, i.e. your function
  certainly does not copy at most `n` characters.

- The `*x` at line 24 dereferences `0`. Is this intentional, or was this meant
  to be `*a`?
- What is the difference between the two variants listed? Why two? What does the
  `_c` suffix indicate?
- The refinement at line 7 seems like a no-op upon a first read, maybe a comment
  would be worth adding to the code sample, since there's no step-by-step
  description of it in the text, e.g.

```
int x = strlen(a); // now: nt_array_ptr<int> a: count(x) ```

- Fig 1 uses `dyn_bounds_cast`, Fig 5 uses `dynamic_bounds_cast`
- The indentation is confusing, e.g. alignment of if, for and else lines 12-15.
- Missing { } surrounding line 25?
- If you can, meaningful names like `src` and `dst` are to be preferred over `a` and
  `b`.

- III.C: transtive (typo)
- III.C, "Bounds Widening": can the join be always computed? this may be a hard
  problem...?
- III.D: LaTeX problem with the name of Theorem 3
- IV: ghost variables in other contexts (e.g. Why3, Dafny) are used for things
  that *do not exist* at run-time, but this doesn't seem to be the case here
- Fig 8, line 1: did you mean `bounds` instead of `count`? I'm confused
- Fig 8, line 8: why null-check `p` again?
- IV.C.: reginos (typo)
- Do your bug report and github links break anonymity?

Requested changes
- clarify relationship between formalization and implementation
- correct and address mistakes, typos, confusing terminology and code snippets
- address pressing questions in the paper.

Review #70B
Overall merit
4. 
Minor revision

Reviewer expertise
2. 
Some familiarity

Paper summary
The paper presents a formalization of Checked C, which extends the C language with new types and annotations with the aim of enforcing memory safety. A key feature of Checked C is that it supports marking specific code regions as "checked", where memory safety is enforced through the use of Checked C types and annotations, whereas other regions can be "unchecked" (i.e., plain C code without memory safety). Any memory safety violation, then, can be blamed to unchecked code.

Concretely, the paper presents an operational semantics of Checked C that extends prior models with support for dynamically bounded and null-terminated arrays, which require reasoning about Checked C's bounds widening capabilities. The semantics relies on fat pointers (i.e., each pointer is enriched with additional data recording its lower/upper bounds) which are used to prevent memory-unsafe accesses in checked code regions. The authors formalized this semantics in Coq and proved that, indeed, any memory violation can be blamed only to unchecked code.

The authors also formalized a compilation pass that translates programs with fat pointers into programs that use lightweight pointers (that is, pointers without metadata). This compilation pass introduces further ghost variables and checks to ensure memory safety. To gain confidence in the correctness of this compilation pass, the authors implemented both the semantics and the compilation in PLT Redex and used random testing to find counterexamples to the compilation pass' correctness. 

Additionally, the authors used the operational semantics Redex model to check whether the semantics matches the actual Checked C implementation. For this, they developed a model-driven random testing approach which they used to find discrepancies between the Checked C implementation and the authors' semantics.

Strengths
- Formal operational semantics of a relevant part of Checked C together with a
  proof for the "blame theorem" (everything formalized in Coq) plus
  formalization of a compilation pass for getting rid of fat pointers

- Operational semantics and compilation pass have executable implementations,
  which are used to increase confidence in the models and non-mechanized
  theorems

- Topic is interesting and the paper is well-written

Weaknesses
- The paper is at times a bit dense and technical

- The testing part could could benefit from additional details

Comments for author
Thanks for submitting this paper to CSF. 

I really enjoyed reading the paper! 

It presents a formalization of an interesting attempt at providing memory safety for C programs. The Checked C operational semantics given in the paper is well explained, and it extends the semantics from [21] with non-trivial extensions to the semantics itself and to the typing judgment to safely support dynamically-sized arrays. 

The use of fat pointers in the original semantics simplifies the enforcement of memory safety and, I expect, also the proof of the blame theorem. By providing a semantic-preserving translation to a language that uses only standard pointers, the authors make the formal model closer to the real Checked-C implementation which uses "standard" pointers.

I appreciated that the operational semantics is formalized in both Coq and Redex and that the proof of the blame theorem is machine-checked in Coq. I liked even more that the Redex executable semantics has been used to check differences between the formal model and the real implementation (even though I have a few concerns on this section; see below).

Overall, I think this is a very strong paper and I recommend its acceptance to CSF.

Below, I list a few minor concerns and comments on the paper:

- The paper builds on the operational semantics from [21]. Even though the
  authors explained at high-level the main extensions w.r.t. [21], it'd be great
  to add a paragraph that discusses what are the main changes from [21] in terms
  of the technical development (if there are any), e.g., are there any new
  challenges that needed to be solved while proving the blame theorem for this
  paper's semantics?

- My interpretation of section IV.C is that the authors have a pen-and-paper
  proof of Theorem 4, and that random testing using the Redex models has been
  used to gain confidence in such a proof. Is this correct?

- While reading the semantics, I found the fact that S-Def and S-DefNull are
  applicable non-deterministically if n is 0 a bit confusing. Only when
  reading the meta-theory section I realized that this is not a concrete issue
  because well-formed heaps are such that $\mathcal{H}(0)$ is never defined. It
  might be worth pointing this out early on. 

- Theorem 1 refers to a program $e$ being well-formed. Unless I've missed
  something, I didn't see such a definition in the paper.

- While I found the idea behind section V very interesting, the current version
  of this section lacks some details that would help in better understanding (1) 
  how the approach works, and (2) the overall scope of the approach. 
  
  For instance, the authors state that, following [19], they try to "exercise
  interesting patterns" by adding "admissible but redundant typing rules" like
  G-ASTR. There are a few points that are unclear here: (1) are these rules
  discovered manually or automatically (starting from the Redex semantics)?, (2)
  are there any guiding principles for coming up with rules that lead to
  interesting cases?

  Later, the authors refer to "generation rules modified to be slightly more
  permissive" to generate "a little" ill-typed terms. Again, are these rules
  obtained automatically or defined manually? If the latter, did you follow any
  methodology to derive such rules? Are these rules the same as the "admissible
  but redundant typing rules" from above?

Requested changes
- Add details in section 5 (see above)
- Clarify the status of Theorem 4

Review #70C
Overall merit
4. 
Minor revision

Reviewer expertise
3. 
Knowledgeable

Paper summary
The paper presents a formal and mostly-machine checked study of a fragment of CheckedC, a recent variant of safe-C family of calculi that avoids fat pointers and allows the incremental conversion of real-world C code bases. The study extends earlier work by Ruef (POST 2019) by also treating dynamically bounded or null-terminated (rather than only statically sized) arrays, presenting an improved formal compilation that inserts appropriate bounds checks without incurring undue overhead using ghost variables, and subjecting the model to random-based testing using PLT redex (the earlier parts are backed up by a formalization in Coq). The main results are standard syntactic progress + preservation results, plus a blame theorem that confirms that the system respects the expected boundary between checked and unchecked ("trusted") code regions. At mentioned below, I believe there is a typo in the phrasing of the blame theorem in the paper (compared to Coq) but it seems easily correctable.

Strengths
- extends a previous formalization of CheckedC by covering extended language features and making the analysis more expressive/precise, uncovering a shortcoming of the "real"world implementation

- very well written, with informative examples and good explanations of (the necessarily rather technical) details. A technical appendix treats additional language concepts (struct etc), operational rules, typing rules, etc.

- source code in Coq and PLT Redex provided; the Coq code does not process under Coq8.14 (and is only claimed to compile under 8.12), but appears well-structured, and at least partially documented. There are a couple minor holes in the proof script, but the explanations are reasonable (and CSF is not ITP/CPP).

Weaknesses
- one of the main theorems seems to have a typo (easily correctable, I think)

- use of two different proof tools somewhat weakens the overall result, but is a reasonable stepping stone to an eventual complete formalization in a single framework.

Comments for author
Despite the emergence of more modern languages, C remains a dominating language in (at least) the (embedded) systems community, and the CheckedC approach presents a new sweet spot in the design space of static analyses/coding standards/... that ensure safety. Hence, subjecting the language to a formal study is eminently worthwhile, and while the present paper is perhaps a little incremental wrt Ruef's recent paper, there are technical and conceptual novelties so I recommend acceptance (assuming the blame theorem can be corrected).

- There appears to be a slight discrepancy between the blame theorem in Coq and the one in the paper: the paper mentions some e', which I believe should be r. Also, the Coq code has a further disjunct m=Unchecked in the conclusion.

- on page 6, section IIIC, paragraph "Pointer Access" mentions that checked pointers cannot be dereferenced in unchecked blocks - this looks funny, shouldn't it be the other way around? The Coq code contains the hypothesis m'=Unchecked -> m Unchecked in various rules of definition well-typed (BoundCheckedC, line 669; rule TyDeref in the code seems closest to the figure's T-DefArr and T-Def in the appendix, although it's a bit concerning that there's no 1-to-1 correspondence of the rules in the code and the paper).

- Furthermore, inspecting the code also suggests that the expression type (line 155) does not contain constructors for function calls (and I don't see a way to define functions either), conditionals, or strlen, and doesn't distinguish between the two forms of casting. All this contradicts figure 2, and should be clarified

- I'd like to have seen a bit more motivation for using PLT redex - what aspects made the use of this tool preferable to formulating the compilation in Coq and using Quickchick to do random testing of the simulation result.

- there's a funny change of line spacing in column 2 of page 6, about 2/3 down.

- even the extended model does not support full CheckedC, and the formalizations do not directly connect to the actual implementation. Could you discuss what the remaining delta is in terms of language features, and whether it might be feasible to grow the current formalization to one that integrates with the CheckedC code for LLVM, at least for existing Coq-formalizations of LLVM such as Vellum?

Requested changes
- confirmation that blame theorem issue is essentially just a typo.
- clarify the policy rules wrt accessing (un)checked pointers in (un)checked regions
- remove, or at least point out and justify the discrepancies between the Coq code and the paper, eg concerning Fig2

@A1Reviewer C3 Dec 2021
Minimal requirements for acceptance ("shepherding conditions"):

- please clarify the formalization status of Theorem 4 and the intended guarantee w.r.t. NULL 
  dereferencing, clarify whether the code examples are intended to be actual C -- cf the discussion
  of  pointer arithmetic and undefined behavior in review 1 -- and tweak them if necessary) 
- please clarify the policy of dereferencing of (un)checked pointers in (un)checked regions as pointed out by two reviewers, and address the discrepancies between the paper's calculus and the Coq formalization
- please include more details on the proposed model-driven random testing approach from section 5 (cf review B)
