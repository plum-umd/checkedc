\section{Appendix}\label{app:main}

\subsection{Converting C to \checkedc}
\label{app:convertctocc}
%
The safety guarantees of Checked C come with certain restrictions. For instance,
as shown below, Checked C programs cannot use address-taken variables in a
bounds expression as the bounds relations may not hold because of possible
modifications through pointers.
% 
\begin{minted}[xleftmargin=30pt, mathescape, escapeinside=||, fontsize=\footnotesize]{c}
...
_Array_ptr<int> p : count (n) = NULL;
|\textcolor{red}{\faTimes}|..,&n,.
\end{minted}
% 
Consequently, converting existing C programs to Checked C might require
refactoring,~\eg eliminate~\inlinecode{&n} from the program above without
changing its functionality.
% 
This might require considerable effort~\cite{duanrefactoring} depending on the
program's complexity.
% 
Recently, Machiry~\etal developed~\threec~\cite{machiry2022c} that tries to
automatically convert a program to Checked C by adding appropriate pointer
annotations.
However, as described in \threec, completely automated conversion
is \emph{infeasible}, and it requires the developer to convert some code regions
manually. 


\subsection{Well-formedness and Subtype}
\label{app:le}
  

\begin{figure}[h]
{\small
  \begin{mathpar}

  \inferrule[]
  {}
  {m \vdash \tint}

  \inferrule[]
  {\xi \wedge m\vdash \tau \\ \xi \le m}
  {m \vdash \tptr{\tallarrayb{\bvar}{\tau}}{\xi}}

  \inferrule[]
  {\xi \wedge m \vdash \tau\\ \xi \le m}
  {m \vdash \tptr{\tau}{\xi}}

  \inferrule[]
  {\xi \wedge m \vdash \tau\\ \xi \le m \\\\ \fv(\overline{\tau})\cup\fv(\tau)\subseteq \overline{x}}
  {m \vdash \tptr{(\tfun{\overline{x}}{\overline{\tau}}{\tau}}{\xi})}
  \end{mathpar}
}
{\footnotesize
\[
\begin{array}{l} 
\tmode \wedge \cmode = \umode \qquad \xi \wedge \umode = \umode
\qquad \cmode \wedge m = m 
\qquad  m_1 \wedge m_2 = m_2 \wedge m_1
\end{array}
\]
}
 \caption{Well-formedness for Nested Pointers}
\label{fig:wftypes}
\end{figure}

\begin{figure}[h]
{\small
  \begin{mathpar}

    \inferrule[]
    {}
    {\Gamma \vdash n}

    \inferrule[]
    {x:\tint \in \Gamma}
    {\Gamma \vdash x + n}

    \inferrule[]
    {\Gamma \vdash b_l\\
    \Gamma \vdash b_h}
  {\Gamma \vdash (b_l,b_h)}

  \inferrule[]
  {}
  {\Gamma \vdash \tint}

  \inferrule[]
  {\Gamma \vdash \bvar \\
  \Gamma \vdash \tau}
  {\Gamma \vdash \tptr{\tallarrayb{\bvar}{\tau}}{m}}

  \inferrule[]
  {\Gamma \vdash \tau}
  {\Gamma \vdash \tptr{\tau}{m}}

  \inferrule[]
  {T \in D}
  {\Gamma \vdash \tptr{\tstruct{T}}{m}}

  \inferrule[]
  {\Gamma \vdash \tau}
  {\Gamma \vdash \tptr{\tau}{m}}

  \inferrule[]
  {\forall \tau_i\in \overline{\tau}\,.\,\Gamma[\forall x\in\overline{x}\,.\, x\mapsto \tint] \vdash \tau_i\\\\
   \Gamma[\forall x\in\overline{x}\,.\, x\mapsto \tint] \vdash \tau }
  {\Gamma \vdash \tfun{\overline{x}}{\overline{\tau}}{\tau}}
  \end{mathpar}
}
 \caption{Well-formedness for Types and Bounds}
\label{fig:wftypesandbounds}
\end{figure}

\Cref{fig:wftypes} defines the well-formedness for nested pointers, guaranteeing that no tainted ($\tmode$) pointer has a checked ($\cmode$) element field.
Mainly, in a nested pointer $\tptr{(... \tptr{\tau}{\xi_2} ...)}{\xi_1}$, $\xi_2\le \xi_1$.
\Cref{fig:wftypesandbounds} defines the well-formedness for bounds appearing in a type, which requires that any bound must be $\tint$ type and has instance in $\Gamma$.
All bound variables appearing in a type must be either an instance of the type environment $\Gamma$,
or a bound variable appearing in a function pointer.

\begin{DIFnomarkup}
\begin{figure}
{\small
\[\hspace*{-1.2em}
\begin{array}{l}
\textcolor{blue}{\text{Bound Inequality and Equality:}}\\[0.3em]
  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
     n \le n' &\Rightarrow& n &\le_{\Theta} & n'\\
     n \le n' &\Rightarrow& x+n &\le_{\Theta} & x+n'\\
     n \le n' \wedge \Theta(x)=\tgez &\Rightarrow& n &\le_{\Theta} & x+n'\\
     \Theta(x)=\teq{b} \wedge b+n\le_{\Theta}b'  &\Rightarrow& x+n & \le_{\Theta} & b'\\
     \Theta(x)=\teq{b}\wedge b'\le_{\Theta}b+n  &\Rightarrow& b' & \le_{\Theta} & x+n\\
     b \le_{\Theta} b' \wedge b' \le_{\Theta} b  &\Rightarrow& b & =_{\Theta} & b'
    \end{array}
  \\[1.5em]
\textcolor{blue}{\text{Type Equility:}}\\
  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
     && \tint & =_{\Theta} & \tint\\
     \omega =_{\Theta} \omega' &\Rightarrow& \tptr{\omega}{\xi} & =_{\Theta} & \tptr{\omega'}{\xi}\\
     \bvar =_{\Theta} \bvar' \wedge  \tau =_{\Theta} \tau'
             &\Rightarrow& \tallarrayb{\bvar}{\tau} & =_{\Theta} & \tallarrayb{\bvar'}{\tau'}\\

    \textit{cond}(\overline{x},\overline{\tau}\to\tau,\overline{y},\overline{\tau'}\to\tau')

 &\Rightarrow& \tfun{\overline{x}}{\overline{\tau}}{\tau} & 
                         =_{\Theta} & \tfun{\overline{y}}{\overline{\tau'}}{\tau'}\\
    \end{array}
  \\[0.7em]
\textcolor{blue}{\text{Subtype:}}\\[0.3em]

  \begin{array}{r@{~}c@{~}l@{~}c@{~}l}
    \tau =_{\Theta} \tau'&\Rightarrow&\tau &\sqsubseteq_{\Theta}& \tau'\\[0.2em]

    && \tptr{\tau}{\tmode}&\sqsubseteq_{\Theta}& \tptr{\tau}{\umode}\\[0.2em]

    0\le_{\Theta} b_l \wedge b_h \le_{\Theta} 1 &\Rightarrow& \tptr{\tau}{m}&\sqsubseteq_{\Theta}& \tarrayptr{b_l}{b_h}{\tau}{m}\\[0.2em]
    b_l \le_{\Theta} 0 \wedge 1 \le_{\Theta} b_h &\Rightarrow& \tarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tptr{\tau}{m}\\[0.2em]
    b_l \le_{\Theta} 0 \wedge 1 \le_{\Theta} b_h &\Rightarrow& \tntarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tptr{\tau}{m}\\[0.2em]
    %% b_l \le b_l' \wedge b_h' \le b_h &\Rightarrow&  \tarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq&  \tarrayptr{b_l'}{b_h'}{\tau}{m}\\[0.6em]
    b_l \le_{\Theta} b_l' \wedge b_h' \le_{\Theta} b_h &\Rightarrow& \tntarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tarrayptr{b_l'}{b_h'}{\tau}{m}\\[0.6em]
    b_l \le_{\Theta} b_l' \wedge b_h' \le_{\Theta} b_h &\Rightarrow& \tallarrayptr{b_l}{b_h}{\tau}{m} &\sqsubseteq_{\Theta}& \tallarrayptr{b_l'}{b_h'}{\tau}{m}
\\[0.2em]
\overline{\tau'}\sqsubseteq_{\Theta}\overline{\tau}\wedge \tau\sqsubseteq_{\Theta}\tau' &\Rightarrow& \tptr{\tfun{\overline{x}}{\overline{\tau}}{\tau}}{\xi} &\sqsubseteq_{\Theta}& \tptr{\tfun{\overline{x}}{\overline{\tau'}}{\tau'}}{\xi}

    \end{array}
\end{array}
  \]
}
{\footnotesize
\[
\begin{array}{l}
n'+n = add(n',n)
\qquad
(x+n')+n = x+add(n',n)\\
\textit{cond}(\overline{x},\tau,\overline{y},\tau')
=\exists\overline{z}\;.\;\overline{x}\cupdot\overline{z}
  \wedge \overline{y}\cupdot\overline{z}
  \wedge \size(\overline{x})=\size(\overline{y})=\size(\overline{z})
\\\qquad\qquad\qquad\qquad\qquad
  \wedge \tau[\overline{z}/\overline{x}]= \tau'[\overline{z}/\overline{x}]
\end{array}
\]
}
  \caption{Type Equality and Subtyping}
  \label{fig:checkc-subtype}
\end{figure}
\end{DIFnomarkup}


In \lang, type equality $\tau=_{\Theta}\tau'$
is a type construct equivalent relation defined by the bound equality ($=_{\Theta}$) in (NT-)array pointer types
and the alpha equivalence of two function types in \Cref{fig:checkc-subtype};
i.e., two (NT-)array pointer types $\tallarrayb{\bvar}{\tau} $ and $ \tallarrayb{\bvar'}{\tau'}$ are equivalent, if 
$\bvar =_{\Theta} \bvar'$ and $\tau=_{\Theta}\tau'$; two function types 
$\tfun{\overline{x}}{\overline{\tau}}{\tau} $ and $ \tfun{\overline{y}}{\overline{\tau'}}{\tau'}$
are equivalent, if we can find a same length (as $\overline{x}$ and $\overline{y}$) variable list $\overline{z}$ that is substituted for $\overline{x}$ and $\overline{y}$ in $\overline{\tau} \to {\tau}$ and $\overline{\tau'} \to {\tau'}$, resp.,
and the substitution results are equal.

 The \textsc{T-CastPtr} rule
permits casting from an expression of type $\tau'$ to a checked pointer when
$\tau' \sqsubseteq \tptr{\tau}{\cmode}$. This subtyping relation
$\sqsubseteq$ is given in Fig.~\ref{fig:checkc-subtype} and is built on the type equality
($\tau =_{\Theta} \tau'\Rightarrow\tau \sqsubseteq_{\Theta} \tau'$). The many
rules ensure the relation is transitive. Most of the rules manage
casting between array pointer types. The rule 
($0\le b_l \wedge b_h \le 1 \Rightarrow \tptr{\tau}{m}\sqsubseteq
\tarrayptr{b_l}{b_h}{\tau}{m}$) permits treating a singleton
pointer as an array pointer with $b_h\le 1$ and $0 \le b_l$.
Two function pointer types are subtyped ($\tptr{\tfun{\overline{x}}{\overline{\tau}}{\tau}}{\xi} \sqsubseteq_{\Theta} \tptr{\tfun{\overline{x}}{\overline{\tau'}}{\tau'}}{\xi}$), 
if the output type are subtyped ($\tau\sqsubseteq_{\Theta}\tau'$) and the argument types are reversely subtyped ($\overline{\tau'}\sqsubseteq_{\Theta}\overline{\tau}$).
%There is another casting rule in \Cref{app:main} stating that
% users are free to cast types in unchecked code regions, since unchecked regions can contain C code.


The subtyping relation given in Fig.~\ref{fig:checkc-subtype} involves
dependent bounds, i.e., bounds that may refer to variables. To decide
premises $b \leq_{\Theta} b'$ in \Cref{fig:checkc-subtype}, we need a decision procedure that accounts for
the possible values of these variables. This process considers
$\Theta$, tracked by the typing judgment, and $\varphi$, the current
stack snapshot (when performing subtyping as part of the type
preservation proof).

Since bounds expressions may
contain variables, determining assumptions like $b_l \leq_{\Theta} b_l'$
requires reasoning about the probable values of these variables'. The type
system uses $\Theta$ to make such reasoning more precise.
$\Theta$ is a map from variables $x$ to
equation predicates $P$, which have the form $P ::= \tgez \;|\; \teq{b}$.
It maps variables to equations that are recorded along the type checking procedure.
If $\Theta$ maps $x$ to $\tgez$, that means that $x \ge 0$;
$\teq{b}$ means that $x$ is equivalent to the bound value $b$ in the current context, 
such as in the type judgment for $e_2$ in Rule \textsc{T-LetInt} and \textsc{T-RetInt}.

$\sqsubseteq$ is parameterized by
$\Theta$, which provides the range of allowed values for a bound
variable; thus, more $\sqsubseteq$ relation is provable. For example,
rule \rulelab{T-LetInt} inserts a predicate $\teq{e_1}$ for variable $x$.
Assume that $e_1$ is equal to $0$,
when $x$ is used as a type variable in type $\tntarrayptr{0}{x}{\tint}{\cmode}$ in $e_2$,
the subtyping relation $\tntarrayptr{0}{x}{\tint}{\cmode} \sqsubseteq
\tntarrayptr{0}{0}{\tint}{\cmode}$ is provable when we know
\code{x}$\teq{0}$.

To capture bound variables in dependent types, the \checkedc subtyping
relation ($\sqsubseteq$) is parameterized by a restricted stack
snapshot $\varphi|_{\rho}$ and the predicate map $\Theta$, where
$\varphi$ is a stack and $\rho$ is a set of
variables. $\varphi|_{\rho}$ means to restrict the domain of $\varphi$
to the variable set $\rho$. Clearly, we have the relation:
$\varphi|_{\rho} \subseteq \varphi$. $\sqsubseteq$
being parameterized by $\varphi|_{\rho}$ refers to that when we
compare two bounds $b \le_{\Theta} b'$, we actually do
$\varphi|_{\rho}(b) \le_{\Theta} \varphi|_{\rho}(b')$ by interpreting the
variables in $b$ and $b'$ with possible values in $\varphi|_{\rho}$.
Let's define a subset relation $\preceq$ for two restricted stack
snapshot $\varphi|_{\rho}$ and $\varphi'|_{\rho}$:


\begin{DIFnomarkup}
\begin{figure*}[t]
{\small
  \begin{mathpar}
   \inferrule[T-ConstU]
       { \neg \cmode(\tau)}
       {\Gamma;\Theta\vdash_u \evalue{n}{\tau} : \tau}
\quad
   \inferrule[T-ConstC]
       {\Theta;\heap;\emptyset \vdash_c n : \tau}
       {\Gamma;\Theta\vdash_c \evalue{n}{\tau} : \tau}
\quad      
   \inferrule[T-Let]
    { x\not\in \fv(\tau') \\
        \Gamma;\Theta \vdash_m e_1 : \tau \\\\
          \Gamma[x\mapsto \tau];\Theta \vdash_m e_2 : \tau'
             }
    {\Gamma;\Theta \vdash_m \elet{x}{e_1}{e_2} : \tau'}
\quad
   \inferrule[T-LetInt]
    { x\in \fv(\tau') \Rightarrow e_1 \in \text{Bound} \\
        \Gamma;\Theta \vdash_m e_1 : \tint \\\\
           \Gamma[x\mapsto \tint];\Theta[x\mapsto \teq{e_1}] \vdash_m e_2 : \tau'
             }
    {\Gamma;\Theta \vdash_m \elet{x}{e_1}{e_2} : \tau'[e_1 / x]}
\quad
   \inferrule[T-RetInt]
    { \Gamma[x\mapsto \tint];\Theta[x\mapsto \teq{n}] \vdash_m e : \tau}
    {\Gamma;\Theta \vdash_m \eret{x}{\evalue{n}{\tint}}{e} : \tau}

    \inferrule[T-Mac]
              {\xi\le m}
              {\Gamma; \Theta \vdash_m \emalloc{\xi}{\omega} : \tptr{\omega}{\xi}}

    \inferrule[T-Add]
              {\Gamma; \Theta \vdash_m e_1 : \tint \\
                \Gamma; \Theta \vdash_m e_2 : \tint}
              {\Gamma; \Theta \vdash_m (e_1 \plus e_2) : \tint }

    \inferrule[T-Ind] 
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tallarrayb{\bvar}{\tau}}{\xi} \\
                \Gamma; \Theta \vdash_m e_2 : \tint \\
                \xi \leq m}              
              {\Gamma; \Theta \vdash_m \estar{(\ebinop{e_1}{e_2})} : \tau}

    \inferrule[T-Assign]
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tau}{\xi} \\
                \Gamma; \Theta \vdash_m e_2 : \tau' \\\\
                \tau'\sqsubseteq_{\Theta} \tau \\
                \xi \leq m}
              {\Gamma; \Theta \vdash_m \eassign{e_1}{e_2} : \tau}

    \inferrule[T-AssignArr]
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tallarrayb{\bvar}{\tau}}{\xi}\\\\
                \Gamma; \Theta \vdash_m e_2 : \tau' \\
                \tau'\sqsubseteq_{\Theta} \tau\\
                \xi \leq m}
              {\Gamma; \Theta \vdash_m \eassign{e_1}{e_2} : \tau}              

   \inferrule[T-IndAssign]
              {\Gamma; \Theta \vdash_m e_1 : \tptr{\tallarrayb{\bvar}{\tau}}{\xi}\\
                \Gamma; \Theta \vdash_m e_2 : \tint \\\\
                \Gamma; \Theta \vdash_m e_3 : \tau' \\
                \tau'\sqsubseteq_{\Theta} \tau \\
                \xi \leq m}
              {\Gamma; \defscope \vdash_m \eassign{(e_1 \plus e_2)}{e_3} : \tau}


  \end{mathpar}
}
% {\footnotesize
% \begin{center}
% $
% \begin{array}{l}
% \fm(e)\triangleq(\exists x\; n\; \tau. e=x+\evalue{n}{\tau}) \vee (\exists n\;\tau. e = \evalue{n}{\tau})
% \\[0.2em]
% \tau[\overline{e} / \overline{x}]\texttt{(with types }\evalue{\overline{x}}{\overline{\tau}}\texttt{)}\triangleq \forall e_i\in\overline{e}\;x_i\in\overline{x}\;\tau_i\in\overline{\tau}\;.\;\tau_i = \tint \wedge (x_i \in \fv(\tau) \Rightarrow \fm(e_i)) \Rightarrow \tau[e_i / x_i]
% \end{array}
% $
% \end{center}
% }
\caption{Remaining \lang Type Rules (extends Fig.~\ref{fig:type-system-1})}
\label{fig:rem-type-system}
\end{figure*}
\end{DIFnomarkup}

\begin{defi}[Subset of Stack Snapshots]
  Given two $\varphi|_{\rho}$ and $\varphi'|_{\rho}$,
  $\varphi|_{\rho} \preceq \varphi'|_{\rho}$, iff for $x\in\rho$ and
  $y$,
  $(x,y) \in \varphi|_{\rho} \Rightarrow (x,y) \in \varphi'|_{\rho}$.
\end{defi}

For every two restricted stack snapshots $\varphi|_{\rho}$ and
$\varphi'|_{\rho}$, such that
$\varphi|_{\rho} \preceq \varphi'|_{\rho}$, we have the following
theorem in \checkedc (proved in Coq):

\begin{thm}[Stack Snapshot Theorem]
  Given two types $\tau$ and $\tau'$, two restricted stack snapshots
  $\varphi|_{\rho}$ and $\varphi'|_{\rho}$, if
  $\varphi|_{\rho}\preceq \varphi'|_{\rho}$, and
  $\tau \sqsubseteq \tau'$ under the parameterization of
  $\varphi|_{\rho}$, then $\tau \sqsubseteq \tau'$ under the
  parameterization of $\varphi'|_{\rho}$.
\end{thm}

Clearly, for every $\varphi|_{\rho}$, we have
$\emptyset \preceq \varphi|_{\rho}$. The type checking stage is a
compile-time process, so $\varphi|_{\rho}$
is $\emptyset$ at the type checking stage. Stack snapshots are needed
for proving type preserving, as variables in bounds expressions are
evaluated away.


\subsection{Other Type Rules and Literal Validity Checks}\label{rem-type}

Here we show the type rules for other \lang operations in Fig.~\ref{fig:rem-type-system}.
Rule \textsc{T-Mac} deals with
$\emalloctext$ operations. There is a well-formedness check to require
that the possible bound variables in $\omega$ must be in the domain of
$\Gamma$ (see Fig.~\ref{fig:wftypesandbounds}). This is similar to the well-formedness assumption of the type environment (Definition~\ref{type-wellformed}). Rule \textsc{T-Add} deals with binary operations whose sub-terms are integer expressions.
The other rules are explained as follows.

\myparagraph{Pointer Access}
%
The \textsc{T-AssignArr} rule examines array assignment operations, returning the type of
pointed-to objects. Rules for pointers for other object types are
similar, such that Rule \textsc{T-Assign} assigns a value to a non-array pointer location.
The condition $\xi\le m$ ensures that checked and unchecked pointers 
can only be dereferenced in checked and unchecked regions, respectively;
The type rules do not attempt to reason whether the access is in bounds;
such check is deferred to the semantics.
Rule \textsc{T-Ind} serves the case for pointer arithmetic. For simplicity, in the \checkedc formalization, we do not allow arbitrary pointer arithmetic. The only pointer arithmetic operations allowed are the forms shown in rules \textsc{T-Ind} and \textsc{T-IndAssign} in Fig.~\ref{fig:rem-type-system}.  The predicate $\tau'\sqsubseteq_{\Theta} \tau$ requires that the value being assigned is a subtype of the pointer type.
The \textsc{T-IndAssign} rule is an extended assignment operation for handling assignments for array/NT-array pointers with pointer arithmetic.

% Subtyping and casting operations are briefly introduced in
% Sec.~\ref{sec:intros}~and~\ref{sec:overview}.  Subtyping is useful in
% static casting operations that allow users to view a pointer in one
% type as another, such as casting an NT-array pointer to an array
% one. \checkedc provides a set of safe static casting operations that
% have no cost in execution.  Moreover, subtyping acts as oracles for
% bound widening and dynamic casting operations; thus, \checkedc is
% different from a complete static array pointer bound system.  For
% example, if $e$ has type $\tau'$ and $\varphi$ is the current stack
% snapshot, the semantics of $\edyncast{\tau}{e}$ does not transition to
% an error state when $\varphi(\tau')\sqsubseteq\varphi(\tau)$.  In a
% function call, for every argument, \lang permits users to input a
% subtype entity and we prove that this does not affect the correctness
% of the program.

\begin{DIFnomarkup}
 \begin{figure}[t]
 {\small

 \begin{mathpar}
   \inferrule
       {}
       {\Theta;\heap;\sigma \vdash_m n : \tint}

   \inferrule
       {}
       {\Theta;\heap;\sigma \vdash_m 0 : \tptr{\omega}{\xi}}

   \inferrule
       {(m = \cmode \Rightarrow \xi \neq \cmode) \\\\ (m=\umode \Rightarrow \xi = \umode)}
       {\Theta;\heap;\sigma \vdash_{\cmode} n : \tptr{\omega}{\tmode}}
  
   \inferrule
       {(\evalue{n}{\tptr{\omega}{\xi}})\in \sigma}
       {\Theta;\heap;\sigma \vdash_m n : \tptr{\omega}{\xi}}


   \inferrule
       {\tptr{\omega'}{\xi'} \sqsubseteq_{\Theta} \tptr{\omega}{\xi} 
            \\ \Theta;\heap;\sigma \vdash_m n : \tptr{\omega'}{\xi'}}
       {\Theta;\heap;\sigma \vdash_m n : \tptr{\omega}{\xi}}

   \inferrule
       { \xi \le m 
     \\\Xi(m,n)=\tau\;(\evalue{\overline{x'}}{\overline{\tau}})\;(\xi,e)
       \\  \overline{x} = \{x|(x:\tint) \in (\overline{x'}:\overline{\tau}) \}}
       {\Theta;\heap;\sigma \vdash_m n : \tptr{(\tfun{\overline{x}}{\overline{\tau}}{\tau})}{\xi}}
  
   \inferrule
       {\neg\funptr(\omega)\\ \xi \le m\\
        \forall i \in [0,\size(\omega)) \;.\;
            \Theta;\heap;(\sigma \cup \{(n:\tptr{\omega}{\xi})) \}\vdash_m \heap(m,n+i)}
       {\Theta;\heap;\sigma \vdash_m n : \tptr{\omega}{\xi}}
 \end{mathpar}
 }
{\footnotesize
\[
\begin{array}{l} 
\funptr(\tfun{\overline{x}}{\overline{\tau}}{\tau}) = \texttt{true}
\qquad
\funptr(\omega) = \texttt{false}\;\;{[\emph{owise}]}
\end{array}
\]
}
 \caption{Verification/Type Rules for Constants}
 \label{fig:const-type}
 \end{figure}
\end{DIFnomarkup}

\myparagraph{Literal Constant Validity}
Rules \textsc{T-ConstU} and \textsc{T-ConstC}
describes type assumptions for literals appearing in a program.
$\cmode(\tau)$ judges that a literal pointer 
in an unchecked region cannot be of a checked type,
which represents an assumption that programmers 
cannot guess a checked pointer address and utilize it in an unchecked region in \systemname.
In rule \textsc{T-ConstC}, we requires a static 
verification procedure for validating a literal pointer, 
which is similar to the dynamic verification process in \Cref{sec:typechecking}. 

The verification process $\Theta;\heap;\sigma \vdash_m n : \tau$ checks (\Cref{fig:const-type})
validate the literal $\evalue{n}{\tau}$, 
where $\heap(m)$ is the initial heap that the literal resides on and
$\sigma$ is a set of literals assumed to be checked.
A global function store $\Xi(m)$ is also required to check the validity of a function pointer.
A valid function pointer should appear in the right store region ($\cmode$ or $\umode$)
and the address stores a function with the right type.
The last rule in \Cref{fig:const-type} describes the validity check for a non-function pointer, 
where every element in the pointer range ($[0,\size(\omega))$) should be well
typed.


\begin{figure*}[t]
{\small
\begin{mathpar}

\inferrule [S-Var]{} {(\varphi,\heap,x)\longrightarrow (\varphi,\heap,\varphi(x))}

    \inferrule[S-DefArrayC]{\heap(\cmode,n)=\evalue{n_a}{\tau_a} \\ 0 \in [n_l,n_h)}
    {(\varphi,\heap,\estar{\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\cmode}}}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tau})}

    \inferrule[S-DefArrayT]{\heap(\umode,n)=\evalue{n_a}{\tau_a} \\ 0 \in [n_l,n_h) 
               \\  \emptyset;\heap ; \emptyset \vdash_{\umode}\evalue{n_a}{\tau}}
    {(\varphi,\heap,\estar{\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\tmode}}}) \longrightarrow (\varphi,\heap,\evalue{n_a}{\tau})}

    \inferrule[S-DefArrayBound]{0 \not\in [n_l,n_h)}
     { (\varphi,\heap,\estar{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{\xi}}}) \longrightarrow (\varphi,\heap,\ebounds)}
\quad
    \inferrule[S-DefNTArrayBound]{0 \notin [n_l,n_h]}
    {(\varphi,\heap,\estar{\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\xi}}}) \longrightarrow (\varphi,\heap,\ebounds)}
\quad
    \inferrule[S-Checked]{}{(\varphi,\heap,\echecked{\overline{x}}{\evalue{n}{\tau}}) \longrightarrow (\varphi,\heap,\evalue{n}{\tau})}

    \inferrule[S-RetEnd]{}{(\varphi,\heap,\ret{x}{\evalue{n}{\tau}}{\evalue{n'}{\tau'}}) \longrightarrow (\varphi,\heap,\evalue{n'}{\tau'})}

        \inferrule[S-Let]{}{(\varphi,\heap,\elet{x}{\evalue{n}{\tau}}{e}) \longrightarrow (\varphi,\heap,\ret{x}{\evalue{n}{\tau}}{e})}

    \inferrule[S-AssignNull]{}
      {(\varphi,\heap,\eassign{\evalue{0}{\tptr{\omega}{\xi}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap,\enull)}

    \inferrule[S-RetCon]{ (\varphi[x\mapsto \evalue{n}{\tau}],\heap,e) \longrightarrow (\varphi',\heap',e')}{(\varphi,\heap,\ret{x}{\evalue{n}{\tau}}{e}) \longrightarrow (\varphi'[x\mapsto \varphi(x)],\heap',\ret{x}{\varphi'(x)}{e'})}

    \inferrule[S-AssignArrC]{\heap(\cmode,n)=\evalue{n_a}{\tau_a}\\ 0 \in [n_l,n_h) }
      {(\varphi,\heap,\eassign{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heapup{\cmode}{n}{\evalue{n_1}{\tau_a}},\evalue{n_1}{\tau})}

    \inferrule[S-AssignArrT]{\heap(\umode,n)=\evalue{n_a}{\tau_a}\\ 0 \in [n_l,n_h) 
               \\ \emptyset;\heap ; \emptyset \vdash_{\umode}\evalue{n_a}{\tau}}
      {(\varphi,\heap,\eassign{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{\tmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heapup{\umode}{n}{\evalue{n_1}{\tau_a}},\evalue{n_1}{\tau})}

    \inferrule[S-AssignArrBound]{0 \not\in [n_l,n_h) }
      {(\varphi,\heap,\eassign{\evalue{n}{\tallarrayptr{n_l}{n_h}{\tau}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap,\ebounds)}

    \inferrule[S-AssignC]{\heap(\cmode,n)=\evalue{n_a}{\tau_a} }
      {(\varphi,\heap,\eassign{\evalue{n}{\tptr{\tau}{\cmode}}}{\evalue{n_1}{\tau_1}}) \longrightarrow (\varphi,\heap[n \mapsto \evalue{n_1}{\tau}],\evalue{n_1}{\tau})}

  \inferrule[S-Malloc]{\varphi(\omega)=\omega_a \\ \mathtt{alloc}(\heap,\xi,\omega_a)=(n,\heap')}
   { (\varphi,\heap,\emalloc{\xi}{\omega}) \longrightarrow (\varphi,\heap',\evalue{n}{\tptr{\omega_a}{\xi}})}

  \inferrule[S-MallocBound]{\varphi(\omega)=\tallarray{n_l}{n_h}{\tau}\\ (n_l \neq 0 \vee n_h \le 0)}
    { (\varphi,\heap,\emalloc{\omega}) \longrightarrow (\varphi,\heap',\ebounds)}

  \inferrule[S-IfNTNotC]{\varphi(x)=\evalue{n}{\tntarrayptr{n_l}{n_h}{\tau}{\cmode}} \\ \heap(\cmode,n)\neq 0\\ 0 < n_h}
             {(\varphi,\heap,\eif{\estar{x}}{e_1}{e_2}) \longrightarrow (\varphi,\heap,e_1)}

    \inferrule[S-IfT]{n \neq 0 }
    {(\varphi,\heap,\eif{\evalue{n}{\tau}}{e_1}{e_2}) \longrightarrow (\varphi,\heap,e_1)}

    \inferrule[S-IfF]{}
    {(\varphi,\heap,\eif{\evalue{0}{\tau}}{e_1}{e_2}) \longrightarrow (\varphi,\heap,e_2)}

    \inferrule[S-Add]{n = n_1 + n_2}
    {(\varphi,\heap,\evalue{n_1}{\tint} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, n)}

    \inferrule[S-AddArr]{n = n_1 + n_2\\ n_l' = n_l - n_2 \\ n_h' = n_h - n_2}
    {(\varphi,\heap,\evalue{n_1}{\tallarrayptr{n_l}{n_h}{\tau}{\xi}} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, \evalue{n}{\tallarrayptr{n_l'}{n_h'}{\tau}{\xi}})}

n    \inferrule[S-AddArrNull]{}
    {(\varphi,\heap,\evalue{0}{\tallarrayptr{n_l}{n_h}{\tau}{\xi}} \plus \evalue{n_2}{\tint}) \longrightarrow (\varphi,\heap, \enull)}

\end{mathpar}

}
\caption{Remaining \lang Semantics Rules (extends Fig.~\ref{fig:type-system-1})}
\label{fig:rem-semantics}
\end{figure*}

A checked pointer checks validity in type step as rule \textsc{T-ConstC},
while a tainted/unchecked pointer does not check for such during the type checking.
Tainted pointers are verified through the validity check in dynamic execution as we mentioned above.

If the literal's type is an integer, an unchecked pointer, or a null
pointer, it is well typed, as shown by the top three rules in
Fig.~\ref{fig:const-type}. However, if it is a checked pointer
$\tptr{\omega}{\cmode}$, we need to ensure that what it points to in
the heap is of the appropriate pointed-to type ($\omega$), and also
recursively ensure that any literal pointers reachable this way are
also well-typed. This is captured by the bottom rule in the figure,
which states that for every location $n+i$ in the pointers' range
%
$[n, n+\size(\omega))$, where $\size$ yields the size of its argument,
  then the value at the location $\heap(n+i)$ is also well-typed.
  However, as heap snapshots can contain cyclic structures (which
  would lead to infinite typing deriviations), we use a scope $\sigma$
  to assume that the original pointer is well-typed when checking the
  types of what it points to. The middle rule then accesses the scope
  to tie the knot and keep the derivation finite, just like in
  \citet{ruef18checkedc-incr}.

\myparagraph{Let Bindings}
%
Rules \textsc{T-Let} and \textsc{T-LetInt} type a $\elettext$ expression, which also admits
type dependency. 
In particular, the result of evaluating a $\elettext$
may have a type that refers to one of its bound variables (e.g., if
the result is a checked pointer with a variable-defined bound). If so, we must substitute away this variable once it goes out of scope (\textsc{T-LetInt}). 
Note that we restrict the expression $e_1$ to syntactically match the
structure of a Bounds expression $b$ (see Fig.~\ref{fig:checkc-syn}).
Rule \textsc{T-RetInt} types a $\erettext$ expression when $x$ is of type $\tint$.
$\erettext$ does not appear in source programs but is introduced by the semantics when
evaluating a let binding (rule \textsc{S-Let} in
Fig.~\ref{fig:rem-semantics}). 

\subsection{Other Semantic Rules}\label{sec:rem-semantics}

Fig.~\ref{fig:rem-semantics} shows the remaining semantic rules for
$\lang$. We explain a selected few rules in this subsection.
% other few low-level semantic rules for variable and dereference and $\emalloctext$ operations in \checkedc. Other operations are defined in the same manner.

\myparagraph{Checked and Tainted Pointer Operations}
Rule \textsc{S-Var} loads the value for $x$ in stack $\varphi$.
Rules \textsc{S-DefArrayC} and \textsc{S-DefArrayT} dereference an $\cmode$ and $\tmode$ array pointer, respectively.
In \lang, the difference between array and NT-array dereference is that the range of $0$ is at $[n_l,n_h)$ not $[n_l,n_h]$, meaning that one cannot dereference the upper-bound position in an array.
Rules \textsc{DefArrayBound} and \textsc{DefNTArrayBound} describe an error case for a dereference operation.
If we are dereferencing an array/NT-array pointer and the mode is $\cmode$ (or $\tmode$), $0$ must be in the range from $n_l$ to $n_h$ (meaning that the dereference is in-bound); if not, the system results in a $\ebounds$ error. Obviously, the dereference of an array/NT-array pointer also experiences a $\enull$ state transition if $n\le 0$.

Rules \textsc{S-Malloc} and \textsc{S-MallocBound} describe the $\emalloctext$ semantics. Given a valid type $\omega_a$ that contains no free variables, $\mathtt{alloc}$ function returns an address pointing at the first position of an allocated space whose size is equal to the size of $\omega_a$ for a specific mode $\xi$, 
and a new heap snapshot $\heap'$ that marks the allocated space for the new allocation. 
If $\xi$ is $\cmode$, the new $\heap$ allocation is in the $\cmode$ region, while $\tmode$ and $\umode$ mode allocation creates new pieces in $\umode$ region.
The $\emalloctext$ is transitioned to the address $n$ with the type ${\tptr{\omega_a}{\xi}}$ and new updated heap. It is possible for $\emalloctext$ to transition to a $\ebounds$ error if the $\omega_a$ is an array/NT-array type $\tallarray{n_l}{n_h}{\tau}$, and either $n_l \neq 0$ or $n_h \le 0$. This can happen when the bound variable is evaluated to a bound constant that is not desired. 

Rules \textsc{S-AssignArrC} and \textsc{S-AssignArrT} are for assignment operations.
\textsc{S-AssignArrC} assigns to an array as long as 0 (the point of
dereference) is within the bounds designated by the pointer's annotation
and strictly less than the upper bound. 
Rule \textsc{S-AssignArrT} is similar to \textsc{S-AssignArrC} for tainted pointers.
Any dynamic heap access of a tainted pointer requires a \textit{verification}.
Performing such a verification equates to performing a literal type check for a pointer constant in \Cref{fig:const-type}.

\myparagraph{Let Bindings}
%
The semantics manages variable scopes using the special $\erettext$
form. \textsc{S-Let} evaluates to a configuration whose expression is
$\ret{x}{\evalue{n}{\tau}}{e})$. We keep $\varphi$ unchanged
and remember $x$ and its new value $\evalue{n}{\tau}$
in $e$'s scope that is defined by the $\erettext$ operation.
Every time when evaluation proceeds on $e$ (rule \textsc{S-RetCon}),
we install the stack value $\evalue{n}{\tau}$ for $x$ in $\varphi$ for the current scope.
After one-step evaluation is completed, 
we store $x$'s change in the result $\erettext$ operation $\ret{x}{\varphi'(x)}{e'})$,
and restore $x$'s outer score value $\varphi(x)$ in $\varphi'$. 
This procedure continues until $e'$ becomes a literal
$n\!:\!\tau$, in which case \textsc{S-RetEnd} removes the $\kw{ret}$ frame and returns
the literal. 

\subsection{Compilation Formalism}\label{appx:comp1}

% \review{
% The first reviewer said:
% \begin{itemize}
% \item I had a really hard time understanding precisely the "formalized" compilation
%   from CoreChkC to CoreC. Specifically: is CoreC intended to model LLVM-IR, or
%   is it a subset of C? Is CheckedC compiling to C or is it a new frontend like
%   `clang`? See remark about undefined behavior (UB) below which got me even more
%   confused. \mwh{CoreC is meant to model CoreChkC but with
%   annotations, and checks removed; it is not meant to model C or LLVM-IR.}
% \item Is the compilation scheme from CoreChkC to CoreC is faithful to the actual
%   compilation scheme of the checkedc-clang compiler to ...? I feel like the
%   paper is missing an actual description of what happens in the compiler to
%   allow us to connect the dots and understand how the formalization illuminates
%   the implementation. \mwh{We did not model compilation on the real
%   implementation; our purpose was to show that annotations in CoreChkC
%   do not necessitate fat pointers in an implementation; that said,
%   our formalization does show how a real implementation can be
%   carried out}
% \item The paper doesn't seem upfront about what is *shown* (theorem in Coq) and what
%   is *tested* (via PLT-Redex), and thus remains a
%   conjecture. \mwh{Updated the intro and the individual sections}
% \item  Missing discussion of why Coq vs. PLT-Redex, effort involved, any plans to
%   formally prove compilation from CoreChkC to CoreC, any hopes of integrating
%   that in the official implementation, etc. \mwh{Added note at the
%   start of section 3.}
% \end{itemize} }

% \review{IV: ghost variables in other contexts (e.g. Why3, Dafny) are used for things
%   that do not exist at run-time, but this doesn't seem to be the case here.}
% \yiyun{Agreed: We changed the name to ``shadow variables'' to avoid confusion}
% \review{Do your bug report and github links break anonymity?}


% \review{From reviewer C:
% \begin{itemize}
% \item I'd like to have seen a bit more motivation for using PLT redex:
%   what aspects made the use of this tool preferable to formulating the
%   compilation in Coq and using Quickchick to do random testing of the
%   simulation result.
%   \mwh{Added some text to the end of III.A and IV.C}
% \yiyun{Some reasons I can think of:\begin{itemize}
%     \item Redex is highly optimized for specifying judgments that are algorithmic. By writing down a typing relation, we can immediately obtain a typechecker
%     that is executable. Same applies for the small-step evaluation relation. Translating the relations into functions in Coq is definitely doable but time-consuming, especially since compilation is embedded as part of the typing rules. It is also hard to see whether the function we define really corresponds to the relation unless we formally prove it. This issue is particularly relevant at the early stage of the development when the compilation rules were buggy and the simulation property was violated often as we added new generator cases.
%     In Redex, we don't have any formal guarantee either, but at least we can more easily see the correspondence because Redex is able to convert the relation into an executable version so we can specify the relation literally. This feature of Redex helped us speed up our development significantly when our compilation rules were constantly changing.
% \end{itemize} }
% \item there's a funny change of line spacing in column 2 of page 6, about 2/3 down.
% \end{itemize}
% }

The main subtlety of compiling \checkedc to Clang/LLVM is to capture the annotations on pointer literals
that track array bound information, which is used in premises
of rules like \textsc{S-DefArrayC} and
  \textsc{S-AssignArrC} to prevent spatial safety violations.
The \checkedc compiler \cite{li22checkedc} inserted additional pointer checks 
for verifying pointers are not null and the bounds are within their limits.
The latter is done by introducing additional shadow variables for storing (NT-)array pointer bound information.

\begin{figure}[t!]
{\small
\hspace*{-0.5em}
\begin{tabular}{|c|c|c|c|}
\hline
& \cmode & \tmode & \umode \\
\hline
& \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} \\
\hline
\cmode & $\estar{x}$ / $\getstar{\cmode}{x}$ 
 & $\texttt{sand\_get}(x)$ / $\getstar{\umode}{x}$ &  $\times$ \\
\hline
\umode & $\times$
 & $\estar{x}$ / $\getstar{\umode}{x}$ &  $\estar{x}$ / $\getstar{\umode}{x}$ \\
\hline
\end{tabular}

}
\caption{Compiled Targets for Dereference}
\label{fig:flagtable}
\end{figure}

In \systemname, context and pointer modes determine the particular heap/function store that a pointer points to,
i.e., $\cmode$ pointers point to checked regions, while $\tmode$ and $\umode$ pointers point to unchecked regions. 
Unchecked regions are associated with a sandbox mechanism that permits exception handling of potential memory failures.
In the compiled LLVM code, pointer access operations have different syntaxes when the modes are different.
\Cref{fig:flagtable} lists the different compiled syntaxes of a deference operation ($\estar{x}$) for the compiler implementation (\textsc{CBox}, stands for \systemname) and formalism (\textsc{Core}, stands for \lang). The columns represent different pointer modes and the rows represent context modes.
For example, when we have a $\tmode$-mode pointer in a $\cmode$-mode region, we compile a deference operation to the sandbox pointer access function ($\texttt{sand\_get}(x)$) accessing the data in the \systemname implementation. In \lang, we create a new deference data-structure on top of the existing $\estar{x}$ operation (in LLVM): $\getstar{m}{x}$. If the mode is $\cmode$, it accesses the checked heap/function store; otherwise, it accesses the unchecked one.

This section shows how \lang deals with pointer modes, mode switching and function pointer compilations, 
with no loss of expressiveness
as the \checkedc contains the erase of annotations in \cite{li22checkedc}.
For the compiler formalism, 
we present a compilation algorithm that converts from
\lang to \elang, an untyped language without metadata
annotations, which represents an intermediate layer we build on LLVM for simplifying compilation. 
In \elang, the syntax for deference, assignment, malloc, function calls are: $\getstar{m}{e}$, $\elassign{m}{e}{e}$, 
$\emalloc{m}{\omega}$, and $\ecall{e}{\overline{e}}$.
The algorithm sheds light on how compilation can be implemented in the real \systemname
  compiler, while eschewing many vital details (\elang has many differences with LLVM IR).

Compilation is defined by extending \lang's
typing judgment as follows:
\[\Gamma;\Theta;\rho \vdash_m e \gg \dot e:\tau\]
There is now a \elang output $\dot e$ and an input $\rho$, which maps
each (NT-)array pointer variable to its mode and
each variable \code{p} to a pair of \emph{shadow
  variables} that keep \code{p}'s up-to-date upper and lower bounds. 
These may differ from the bounds in \code{p}'s type due to bounds
widening.\footnote{Since lower bounds are never widened, the
  lower-bound shadow variable is unnecessary; we include it for uniformity.} 

% When $\Gamma$,$\Theta$ and $\rho$ are all empty, we write $e \gg \dot e$ rather than the
% complete judgment, implicitly assuming that $e$ is a well-typed and closed
% term.

We formalize rules for this judgment in PLT Redex~\cite{pltredex},
following and extending our Coq development for \lang. To give
confidence that compilation is correct, we use Redex's property-based
random testing support to show that compiled-to $\dot e $ simulates
$e$, for all $e$.

% We developed a \checkedc compiler to compile a \checkedc program to a C program.
% \mwh{We formalized compilation from CoreChkC to a version of CoreChkC
%   but with the metadata removed, right? This is not a Checked C
%   compiler. You go on to see stuff about CompCert, CLight,
%   etc. This is confusing. We should be talking about how this relates
%   to what was just presented. Pick definitive names for things. }
% Given a \checkedc program $e$, we build a compilation process ($\gg$), such that $e \gg \dot e$, where $\dot e$ is the corresponding C program for $e$ in A-normal form (ANF). 
% The compilation process ($\gg$) relies on the type checking step $\Gamma;\Theta\vdash_m e:\tau$. 
% Especially, it relies on $\Gamma$ to provide the type information for variables in $e$. 
% We utilize CompCert/CLight syntax and semantics \cite{Leroy:2009:FVC:1666192.1666216,Blazy2009} as our translation target language.
%  Besides, we defined two data structures in the CompCert format for representing $\enull$ and $\ebounds$ states.
% We write $\xrightarrow{c}$ for the semantics of CLight. \mwh{Of our
%   target language, which I presume does not have all the features that
%   CLight has. Should mention up front that we did all of this in PLT Redex.}

\myparagraph{Approach}
%
Here, we explain the rules for compilation by
examples, using a C-like syntax; the complete rules are given in
\cite{checkedc-tech-report}.
Each rule performs up to three tasks: (a) conversion of $e$ to
A-normal form; (b) insertion of dynamic checks and bound widening expressions; 
and (c) generate right pointer accessing expressions based on modes.
%
A-normal form conversion is straightforward: compound expressions are managed by storing results of subexpressions into temporary variables,
as in the following example.

{\vspace*{-0.5em}
{\small
\begin{center}
$
\begin{array}{l}
$\code{let y=(x+1)+(6+1)}$
\;
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\;
\begin{array}{l}
$\code{let a=x+1;}$\\
$\code{let b=6+1;}$\\
$\code{let y=a+b}$\\
\end{array}
\end{array}
$
\end{center}
}
}

This simplifies the management of effects from subexpressions. The
next two steps of compilation are more interesting.
We state them based on different \lang operations.

\begin{figure}[t!]
  \begin{small}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
int deref_array(n : int,
     p :  $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\cmode}$,
     q : $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\tmode}$) {
  /* $\color{purple!40!black}\rho$(p) = p_lo,p_hi,p_m */
  /* $\color{purple!40!black}\rho$(q) = q_lo,q_hi,q_m */
    * p;
    * q = 1;
}
...
/* p0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\cmode}$ */
/* q0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\tmode}$ */
deref_array(5, p0, q0);
    \end{lstlisting}
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
deref_array(int n, int* p, int * q) {
  //m is the current context mode
  let p_lo = 0; let p_hi = n; 
  let q_lo = 0; let q_hi = n; 
  /* runtime checks */
  assert(p_lo <= 0 && 0 <= p_hi);
  assert(p != 0);
  *(mode(p) $\wedge$ m,p);
  verify(q, not_null(m, q_lo, q_hi) 
             && q_lo <= 0 && 0 <= q_hi);
  *(mode(q) $\wedge$ m,q)=1;
}
...
deref_array(5, p0, q0);
    \end{lstlisting}
\end{small}
    \caption{Compilation Example for Dependent Functions}
\label{fig:compilationexample1}
\end{figure}

% \review{Fig 9: if this is actual C code, then your null-check at line 6 will be
%   eliminated by the compiler. At line 3, you performed a pointer addition, which
%   is only defined when `p` is non-null. So, either `p` is non-null, and the
%   NULL-check can be eliminated; or, `p` is NULL, but line 3 was undefined
%   behavior, meaning the compiler is allowed to do anything, notably eliminate
%   the NULL-check. This is where I am super confused, and either:
%   - CoreC is not really the C language, and has different semantics...? but is
%     this well-defined in the context of LLVM?
%   - there is a problem that was not caught by the PLT-Redex-based testing.
% \yiyun{We have clarified at the start of IV that CoreC is an untyped
%   variant of CoreChkC, and does not aim to represent C per se, or LLVM
%   IR. We aimed to avoid confusion by rewriting the examples in a way that is more closely
%     related to the syntax presented in Fig 3. Pointer arithmetic between 0 and a non-zero
%     index is always valid because CoreC there is technically only
%     integer arithmetic.}}

\myparagraph{Pointer Accesses and Modes}
%
In every declaration of a pointer,
if the poniter is an (NT-)array,
we first allocate two \emph{shadow variables}
to track the lower and upper bounds which are potentially changed for pointer arithmetic and NT-array bound widening.
Each $\cmode$-mode NT-array pointer variable is associated with its type information in a store.
Additionally, we place bounds and null-pointer checks, such as the line 6 and 7 in \Cref{fig:compilationexample1}.
In addition, in the formalism, before every use of a tainted pointer (\Cref{fig:compilationexample1} line 9 and 10), 
there is an inserted verification step similar to \Cref{fig:const-type},
which checks if a pointer is well defined in the heap (\code{not_null}) and the spatial safety.
Predicate \code{not_null} checks that every element in the pointer's range (\code{p_lo} and \code{p_hi}) is well defined in the heap.  
The modes in compiled deference (\code{*(mode(p) }$\wedge$\code{ m,p)})
 and assignment (\code{*(mode(q) }$\wedge$\code{ m,q)=1}) operations 
are computed based on the meet 
operation ($\wedge$) of the pointer mode (e.g. \code{mode(p)}) and the current context mode (\code{m}).

\myparagraph{Checked and Unchecked Blocks}
%
In the \systemname implementation,
$\euncheckedtext$ and $\echeckedtext$ blocks 
are compiled as context switching functions provided by sandbox.
$\eunchecked{\overline{x}}{e}$ is compiled to 
$\texttt{sandbox\_call}(\overline{x},e)$, where we call the sandbox 
to execute expression $e$ with the arguments $\overline{x}$.
$\echecked{\overline{x}}{e}$ is compiled to 
$\texttt{callback}(\overline{x},e)$, where we perform 
a \texttt{callback} to a checked block code $e$ inside a sandbox.
In \systemname, we adopt an aggressive execution scheme that
directly learns pointer addresses from compiled assembly to make the $\code{_Callback}$ happen.
In the formalism, we rely on the type system to 
guarantee the context switching without creating the extra function calls for simplicity.

%Fig.~\ref{fig:compilationexample} shows how an invocation of
%\code{strlen} on a null-terminated string is compiled into C
%code. Each dereference of a checked pointer requires a null check
%(See \textsc{S-DefNull} in Fig.~\ref{fig:semantics}), which the
%compiler makes explicit: Line~$3$ of the generated code has the null
%check on pointer \code{p} due to the \code{strlen},
%  and a similar check happens
%  at line~$8$ due to the pointer arithmetic on \code{p}.
%Dereferences also require bounds checks: line~$2$ checks \code{p} is
%in bounds before computing \code{strlen(p)}, while line~$10$ does
%likewise before computing \code{*(p+1)}.

\myparagraph{Function Pointers and Calls}
%
Function pointers are managed similarly to normal pointers,
but we insert checks to check if the pointer address is not null in 
the function store instead of heap, and whether or not the type is correctly represented, 
for both $\cmode$ and $\tmode$ mode pointers 
\footnote{$\cmode$-mode pointers are checked once in the beginning and $\tmode$-mode pointers are checked every time when use}.
The compilation of function calls (compiling to $\elcall{m}{e}{\overline{e}}$) 
is similar to the manipulation of pointer access operations in \Cref{fig:flagtable}.
For compiling dependent function calls,
\Cref{fig:compilationexample1} provides a hint.
Notice that the bounds for the array pointer \code{p} are not passed as
arguments. Instead, they are initialized according to \code{p}'s
type---see line~4 of the original \lang program at the top of the figure.
Line~$3$ of the generated code
sets the lower bound  to \code{0} and the
upper bound to \code{n}.

\subsection{Constraints and Metatheory}
\label{sec:meta}

Here, we first show some Well-formedness and consistency definitions that are required in \Cref{sec:theorem}, and then show the simulation theorem for the \lang compiler.
Type soundness relies on several \emph{well-formedness}:

\begin{definition}[Type Environment Well-formedness]\label{type-wellformed}
A type environment $\Gamma$ is well-formed if every variable mentioned as type bounds in $\Gamma$ are bounded by $\tint$ typed variables in $\Gamma$.
\end{definition}

\begin{definition}[Heap Well-formedness]
For every $m$, A heap $\heap$ is well-formed if (i) $\heap(m,0)$ is undefined, and
(ii) for all $\evalue{n}{\tau}$ in the range of $\heap(m)$, type $\tau$
contains no free variables. 
\end{definition}

\begin{definition}[Stack Well-formedness]
A stack snapshot $\varphi$ is well-formed if
for all $\evalue{n}{\tau}$ in the range of $\varphi$, type $\tau$
contains no free variables. 
\end{definition}

We also need to introduce a notion of
\emph{consistency}, relating heap environments before and after a
reduction step, and type environments, predicate sets, and stack
snapshots together.

\begin{definition}[Stack Consistency]
A type environment $\Gamma$, variable predicate set $\Theta$, and
stack snapshot $\varphi$ are consistent---written $\Gamma;\Theta\vdash
\varphi$---if for every variable $x$, $\Theta(x)$ is defined implies
$\Gamma(x) = \tau$ for some $\tau$ and 
$\varphi(x) =\evalue{n}{\tau'}$ for some $n,\tau'$ where $\tau' \sqsubseteq_{\Theta} \tau$. 
\end{definition}

\begin{definition}[Checked Stack-Heap Consistency]
A stack snapshot $\varphi$ is consistent with heap $\heap$---written $\heap \vdash \varphi$---if
for every variable $x$, $\varphi(x)= \evalue{n}{\tau}$ with $\mode(\tau)=\cmode$ implies $\emptyset;\heap(\cmode);\emptyset \vdash_{\cmode} n:\tau$.
\end{definition}

\begin{definition}[Checked Heap-Heap Consistency]
A heap $\heap'$ is consistent with $\heap$---written $\heap \triangleright \heap'$---if
for every constant $n$, $\emptyset;\heap;\emptyset \vdash_{\cmode} n:\tau$ implies $\emptyset;\heap';\emptyset \vdash_{\cmode} n:\tau$.
\end{definition}

We formalize both the compilation procedure and the simulation
theorem in the PLT Redex model we developed for \lang (see Sec.~\ref{sec:syntax}),
and then attempt to falsify it via Redex's support for random
testing. Redex allows us
  to specify compilation as logical rules (an extension
  of typing), but then execute it algorithmically to
  automatically test whether simulation holds. This process revealed
  several bugs in compilation and the theorem statement.
%
  % us gain confidence that our original pen and paper proof of
  % simulation remains true with the addition of variable bounds. }
We ultimately plan to prove simulation in the Coq model.

%Turning to the simulation theorem: We first introduce notation
%used to specify the theorem.
We use the notation $\gg$ to
indicate the \emph{erasure} of stack and heap---the rhs is the same as
the lhs but with type annotations removed:
\begin{equation*}
  \begin{split}
    \heap  \gg & \dot \heap \\
    \varphi \gg & \dot \varphi
  \end{split}
\end{equation*}
In addition, when $\Gamma;\emptyset\vdash
\varphi$ and $\varphi$ is well-formed, we write $(\varphi,\heap,e) \gg_m (\dot \varphi, \dot \heap,
\dot e)$ to denote $\varphi \gg \dot \varphi$, $\heap \gg \dot \heap$
and $\Gamma;\Theta;\emptyset \vdash_m e \gg \dot e : \tau$ for some $\tau$ respectively. $\Gamma$ is omitted from the notation since the well-formedness of $\varphi$ and its consistency with respect to $\Gamma$ imply that $e$ must be closed under $\varphi$, allowing us to recover $\Gamma$ from $\varphi$.
Finally, we use $\xrightarrow{\cdot}^*$ to denote the transitive closure of the
reduction relation of $\elang$. Unlike the $\lang$, the semantics of
$\elang$ does not distinguish checked and unchecked regions.



\begin{figure}[t]
{\small
\[
\begin{array}{c}
\begin{tikzpicture}[
            > = stealth, % arrow head style
            shorten > = 1pt, % don't touch arrow head to node
            auto,
            node distance = 3cm
        ]

\begin{scope}[every node/.style={draw}]
    \node (A) at (0,1.5) {$\varphi_0,\heap_0, e_0$};
    \node (B) at (4,1.5) {$\varphi_1, \heap_1 ,e_1$};
    \node (C) at (0,0) {$\dot \varphi_0, \dot \heap_0 ,\dot e_0$};
    \node (D) at (4,0) {$\dot \varphi_1, \dot \heap_1, \dot e_1$};
    \node (E) at (2,-1.5) {$\dot \varphi,\dot \heap ,\dot e$};
\end{scope}
\begin{scope}[every edge/.style={draw=black}]

    \path [->] (A) edge node {$\longrightarrow_{\cmode}$} (B);
    \path [<->] (A) edge node {$\gg$} (C);
    \path [<->] (B) edge node {$\gg$} (D);
    \path [dashed,<->] (C) edge node {$\sim$} (D);
    \path [dashed,->] (C) edge node {$\xrightarrow{\cdot}^*$} (E);
    \path [dashed,->] (D) edge node[above] {$\xrightarrow{\cdot}^*$} (E);
\end{scope}

\end{tikzpicture}
\end{array}
\]
}
\caption{Simulation between \lang and \elang }
\label{fig:checkedc-simulation-ref}
\end{figure}


Fig.~\ref{fig:checkedc-simulation-ref} gives an overview of 
the simulation theorem.\footnote{We ellide the  possibility of $\dot e_1$ evaluating to $\ebounds$ or $\enull$ in the diagram for readability.} The simulation theorem is specified in a way
that is similar to the one by~\citet{merigoux2021catala}.

An ordinary simulation property would
replace the middle and bottom parts of the figure with the
following: \[(\dot \varphi_0, \dot \heap_0, \dot e_0) 
  \xrightarrow{\cdot}^* (\dot \varphi_1, \dot \heap_1, \dot e_1)\]
Instead, we relate two erased configurations using the relation $\sim$,
which only requires that the two configurations will eventually reduce
to the same state.


% The two theorems are translation preservation and simulation. We donate $\xrightarrow{c}$ as the transition semantics of CLight.
\begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
if $\heap_0 \vdash \varphi_0$, $(\varphi_0,\heap_0,e_0)\gg_c (\dot \varphi_0,\dot \heap_0, \dot e_0)$,
and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
\rightarrow_c (\varphi_1, \heap_1, r_1)$, then the following facts hold:

\begin{itemize}

\item if there exists $e_1$ such that $r=e_1$ and $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
$(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\varphi,\dot \heap,\dot e)$ and $(\dot
\varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
\dot \heap,\dot e)$.

\item if $r_1 = \ebounds$ or $\enull$, then we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
\varphi_1$, $\heap_1 \gg \dot \heap_1$.

\end{itemize}
\end{thm}


% when $r_1 = e_1$ for
% some $e_1$ and
% $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.

% \begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
% For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
% if $\emptyset;\emptyset;\emptyset \vdash_\cmode e_0 \gg \dot e_0 :\tau_0$,
% and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
% \rightarrow_\cmode (\varphi_1, \heap_1, r_1)$, when $r_1 = e_1$ for
% some $e_1$ and
% $\emptyset;\emptyset;\emptyset \vdash_\cmode e_1 \gg \dot e_1 :\tau_1$ where $\tau_1 \sqsubseteq \tau_0$
% , then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.
% \end{thm}

As our random generator never generates
$\euncheckedtext$ expressions (whose behavior could be undefined), we can only test a the simulation theorem 
as it relates to checked code. This limitation makes it
unnecessary to state the other direction of the simulation theorem
where $e_0$ is stuck, because Theorem~\ref{thm:progress} guarantees
that $e_0$ will never enter a stuck state if it is well-typed in
checked mode.

The current version of the Redex model has been tested against $21500$
expressions with depth less than $12$. Each expression can
reduce multiple steps, and we test simulation between every two
adjacent steps to cover a wider range of programs, particularly the
ones that have a non-empty heap.

\subsection{Additional Program evaluations}\label{appx:add-prog-eval}

Here, we provide the description of additional program evaluations.

\myparagraph{parsons}
Parsons is annotated comprehensively in two variants parsons\_wasm and parsons\_tainted. parsons\_wasm has most of its input parsing functions moved into the sandbox, whilst having all its pointers marked as tainted. These sandboxed functions interact with the checked region by making indirect calls through RLBOX's callback mechanism. However, with parsons\_tainted, we do not move any of the functions to the sandbox but still mark all the pointers as tainted. The test suite itself consists of 328 tests comprehensively testing the JSON parser's functionality. Benchmarks for both of these forks are recorded using the mean difference between the \systemname and generic-C/checked-C variants when executing 10 consecutive iterations of the test suite. parsons\_wasm expectedly shows 200/266\% runtime overhead when evaluated against checked-c and generic-c respectively due to the performance limitation of WebAssembly. However, evaluating parsons\_tainted against checked-c shows \systemname to be faster because \systemname by itself performs lighter run-time-instrumentation on tainted pointers as compared to the run-time bounds checking performed on checked pointers by checked-c. Furthermore, we only see an average peak memory of 9.5 KiB as compared to the anticipated 82 KiB overhead as Valgrind does not consider the WASM Shadow memory allocated to the tainted pointers.

\myparagraph{LibPNG}
\systemname changes for libPNG is narrow in scope and begins with the encapsulation CVE-2018-144550 and a buffer overflow in compare\_read(). However, we also annotate sections of Lib-png that involve reading, writing, and image processing (interlace, intrapixel, etc) on user-input image data as tainted. That is, rows of image bytes are read into tainted pointers and the taintedness for the row\_bytes is propagated throughout the program. All our changes extend to the png2pnm and pnm2png executables. To evaluate png2pnm, we take the mean of 10 iterations of a test script that runs png2pnm on 52 png files located within the libpng's pngsuite. To test pnm2png, we take the mean of 10 iterations of pnm2png in converting a 52MB 5184x3456 pixels large pnm image file to png. Valgrind's reported lower Heap space consumption for \systemname converted code is due to the discounted space consumed on the heap by the Sandbox's shadow memory. Consequently, when evaluating pnm2png, \systemname's heap consumption was 52 MB lower as the entire image was loaded onto the shadow memory.  

\myparagraph{MicroHTTPD}
MicroHTTPD demonstrates the practical difficulties in converting a program to \systemname. Our conversion for this program was aimed at sandboxing memory vulnerabilities CVE-2021-3466 and CVE-2013-7039. CVE-2021-3466 is described as a vulnerability from a buffer overflow that occurs in an unguarded "memcpy" which copies data into a structure pointer (struct MHD\_PostProcessor pp) which is type-casted to a char buffer (char *kbuf = (char *) \&pp[1]). Our changes would require making the "memcpy" safe by marking this pointer as tainted. However, this would either require marshaling the data pointed by this structure (and its sub-structure pointer members) pointer or would require marking every reference to this structure pointer as tainted, which in turn requires every pointer member of this structure to be tainted. Marshalling data between structure pointers is not easy and demands substantial marshaling code due to the spatial non-linearity of its pointer members unlike a char*. This did not align with our conversion goals which were aimed at making minimal changes. Consequently, the above CVE stands un-handled by \systemname.  Our changes for CVE-2013-7039 involve marking the user input data arguments of this function as tainted pointers and in the interests of seeking minimal conversion changes, we do not propagate the tainted-ness on these functions. Following up on the chronological impossibility of sandboxing bugs before they are discovered and the general programmer intuiting, we moved many of the core internal functions (like MHD\_str\_pct\_decode\_strict\_() and MHD\_http\_unescape()) into the sandbox. 

\myparagraph{Tiny-bignum}
Due to its small size and simplicity, \systemname changes for Tiny-bignum was chosen to be comprehensive. Furthermore, bignum\_to\_string() was moved to the sandbox due to a memory-unsafe use of sprintf(). Given that significant \systemname conversion efforts is attributed to understanding the source-code and finding the precise extent to which we choose to propagate the taintedness or to stop and give up to marshalling the data between regions, Tiny-bignum only required 4 hours. The evaluation was performed on Tiny-bignum's test suite consisting of 4 Test cases, each of which test the functionality to scale on big numbers subject to all of the supported unary and binary operations.  





