\section{Compilation}\label{sec:compilation}

% \review{
% The first reviewer said:
% \begin{itemize}
% \item I had a really hard time understanding precisely the "formalized" compilation
%   from CoreChkC to CoreC. Specifically: is CoreC intended to model LLVM-IR, or
%   is it a subset of C? Is CheckedC compiling to C or is it a new frontend like
%   `clang`? See remark about undefined behavior (UB) below which got me even more
%   confused. \mwh{CoreC is meant to model CoreChkC but with
%   annotations, and checks removed; it is not meant to model C or LLVM-IR.}
% \item Is the compilation scheme from CoreChkC to CoreC is faithful to the actual
%   compilation scheme of the checkedc-clang compiler to ...? I feel like the
%   paper is missing an actual description of what happens in the compiler to
%   allow us to connect the dots and understand how the formalization illuminates
%   the implementation. \mwh{We did not model compilation on the real
%   implementation; our purpose was to show that annotations in CoreChkC
%   do not necessitate fat pointers in an implementation; that said,
%   our formalization does show how a real implementation can be
%   carried out}
% \item The paper doesn't seem upfront about what is *shown* (theorem in Coq) and what
%   is *tested* (via PLT-Redex), and thus remains a
%   conjecture. \mwh{Updated the intro and the individual sections}
% \item  Missing discussion of why Coq vs. PLT-Redex, effort involved, any plans to
%   formally prove compilation from CoreChkC to CoreC, any hopes of integrating
%   that in the official implementation, etc. \mwh{Added note at the
%   start of section 3.}
% \end{itemize} }

% \review{IV: ghost variables in other contexts (e.g. Why3, Dafny) are used for things
%   that do not exist at run-time, but this doesn't seem to be the case here.}
% \yiyun{Agreed: We changed the name to ``shadow variables'' to avoid confusion}
% \review{Do your bug report and github links break anonymity?}


% \review{From reviewer C:
% \begin{itemize}
% \item I'd like to have seen a bit more motivation for using PLT redex:
%   what aspects made the use of this tool preferable to formulating the
%   compilation in Coq and using Quickchick to do random testing of the
%   simulation result.
%   \mwh{Added some text to the end of III.A and IV.C}
% \yiyun{Some reasons I can think of:\begin{itemize}
%     \item Redex is highly optimized for specifying judgments that are algorithmic. By writing down a typing relation, we can immediately obtain a typechecker
%     that is executable. Same applies for the small-step evaluation relation. Translating the relations into functions in Coq is definitely doable but time-consuming, especially since compilation is embedded as part of the typing rules. It is also hard to see whether the function we define really corresponds to the relation unless we formally prove it. This issue is particularly relevant at the early stage of the development when the compilation rules were buggy and the simulation property was violated often as we added new generator cases.
%     In Redex, we don't have any formal guarantee either, but at least we can more easily see the correspondence because Redex is able to convert the relation into an executable version so we can specify the relation literally. This feature of Redex helped us speed up our development significantly when our compilation rules were constantly changing.
% \end{itemize} }
% \item there's a funny change of line spacing in column 2 of page 6, about 2/3 down.
% \end{itemize}
% }

The main subtlety of compiling \checkedc to Clang/LLVM is to capture the annotations on pointer literals
that track array bound information, which is used in premises
of rules like \textsc{S-DefArray} and
  \textsc{S-AssignArr} to prevent spatial safety violations.
The \checkedc compiler \cite{li22checkedc} inserted additional pointer checks 
for verifying pointers are not null and the bounds are within their limits. \lc{what does it mean by bounds within limits?}
The latter is done by introducing additional shadow variables for storing (NT-)array pointer bound information.

\begin{figure}[t!]
{\small
\hspace*{-0.5em}
\begin{tabular}{|c|c|c|c|}
\hline
& \cmode & \tmode & \umode \\
\hline
& \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} & \textsc{CBox} / \textsc{Core} \\
\hline
\cmode & $\estar{x}$ / $\getstar{\cmode}{x}$ 
 & $\texttt{sand\_get}(x)$ / $\getstar{\umode}{x}$ &  $\times$ \\
\hline
\umode & $\times$
 & $\estar{x}$ / $\getstar{\umode}{x}$ &  $\estar{x}$ / $\getstar{\umode}{x}$ \\
\hline
\end{tabular}

}
\caption{Compiled Targets for Dereference}
\label{fig:flagtable}
\end{figure}

In \systemname, context and pointer modes determine the particular heap/function store that a pointer points to,
i.e., $\cmode$ pointers point to checked regions, while $\tmode$ and $\umode$ pointers point to unchecked regions. \lc{context mode doesn't determine where pointer points to but check if it is valid to point to that region?}
Unchecked regions are associated with a sandbox mechanism that permits exception handling of potential memory failures.
In the compiled LLVM code, pointer access operations have different syntaxes when the modes are different. \lc{should we explain why we use different syntax instead of compiling the corresponding heap to llvm}
\Cref{fig:flagtable} lists the different compiled syntaxes of a deference operation ($\estar{x}$) for the compiler implementation (\textsc{CBox}, stands for \systemname) and formalism (\textsc{Core}, stands for \lang). The columns represent different pointer modes and the rows represent context modes.
For example, when we have a $\tmode$-mode pointer in a $\cmode$-mode region, we compile a deference operation to the sandbox pointer access function ($\texttt{sand\_get}(x)$) accessing the data in the \systemname implementation. In \lang, we create a new deference data-structure on top of the existing $\estar{x}$ operation (in LLVM): $\getstar{m}{x}$. If the mode is $\cmode$, it accesses the checked heap/function store; otherwise, it accesses the unchecked one.

This section shows how \lang deals with pointer modes, mode switching and function pointer compilations, 
with no loss of expressiveness
as the \checkedc contains the erase of annotations in \cite{li22checkedc} and \Cref{appx:comp1}.
For the compiler formalism, 
we present a compilation algorithm that converts from
\lang to \elang, an untyped language without metadata
annotations, which represents an intermediate layer we build on LLVM for simplifying compilation. 
In \elang, the syntax for deference, assignment, malloc, function calls are: $\getstar{m}{e}$, $\elassign{m}{e}{e}$, 
$\emalloc{m}{\omega}$, and $\elcall{m}{e}{\overline{e}}$. \lc{strlen?}
The algorithm sheds
  light on how compilation can be implemented in the real Checked C
  compiler, while eschewing many vital details (\elang has many 
  differences with LLVM IR).


%This section shows how \systemname deals with 
%annotations can be safely erased: using static information a compiler
%can insert code to manage and check bounds metadata, with no loss of
%expressiveness. We present a compilation algorithm that converts from
%\lang to \elang, an untyped language without metadata
%annotations. The syntax and semantics \elang
  %closely mirrors that of \lang; it differs only in that literals lack
  %type annotations and its operational rules perform no
  %bounds and null checks, which are instead inserted during
  %compilation. Our compilation algorithm is evidence that \lang's
  %semantics, despite its apparent use of fat pointers, faithfully
  %represents Checked C's intended behavior. The algorithm also sheds
  %light on how compilation can be implemented in the real Checked C
  %compiler, while eschewing many important details (\elang has many 
  %differences with LLVM IR).

Compilation is defined by extending \lang's
typing judgment as follows:
\[\Gamma;\Theta;\rho \vdash_m e \gg \dot e:\tau\]
There is now a \elang output $\dot e$ and an input $\rho$, which maps
each (NT-)array pointer variable to its mode and
each variable \code{p} to a pair of \emph{shadow
  variables} that keep \code{p}'s up-to-date upper and lower bounds. 
These may differ from the bounds in \code{p}'s type due to bounds
widening.\footnote{Since lower bounds are never widened, the
  lower-bound shadow variable is unnecessary; we include it for uniformity.} 
\lc{These two sentences are confusing. }

% When $\Gamma$,$\Theta$ and $\rho$ are all empty, we write $e \gg \dot e$ rather than the
% complete judgment, implicitly assuming that $e$ is a well-typed and closed
% term.

We formalize rules for this judgment in PLT Redex~\cite{pltredex},
following and extending our Coq development for \lang. To give
confidence that compilation is correct, we use Redex's property-based
random testing support to show that compiled-to $\dot e $ simulates
$e$, for all $e$.

% We developed a \checkedc compiler to compile a \checkedc program to a C program.
% \mwh{We formalized compilation from CoreChkC to a version of CoreChkC
%   but with the metadata removed, right? This is not a Checked C
%   compiler. You go on to see stuff about CompCert, CLight,
%   etc. This is confusing. We should be talking about how this relates
%   to what was just presented. Pick definitive names for things. }
% Given a \checkedc program $e$, we build a compilation process ($\gg$), such that $e \gg \dot e$, where $\dot e$ is the corresponding C program for $e$ in A-normal form (ANF). 
% The compilation process ($\gg$) relies on the type checking step $\Gamma;\Theta\vdash_m e:\tau$. 
% Especially, it relies on $\Gamma$ to provide the type information for variables in $e$. 
% We utilize CompCert/CLight syntax and semantics \cite{Leroy:2009:FVC:1666192.1666216,Blazy2009} as our translation target language.
%  Besides, we defined two data structures in the CompCert format for representing $\enull$ and $\ebounds$ states.
% We write $\xrightarrow{c}$ for the semantics of CLight. \mwh{Of our
%   target language, which I presume does not have all the features that
%   CLight has. Should mention up front that we did all of this in PLT Redex.}

\subsection{Approach}

Due to space constraints, we explain the rules for compilation by
examples, using a C-like syntax; the complete rules are given in
\iftr
Appendix~\ref{appx:comp1}.
\else
the supplemental report~\cite{checkedc-tech-report}.
\fi
Each rule performs up to three tasks: (a) conversion of $e$ to
A-normal form; (b) insertion of dynamic checks and bound widening expressions; 
and (c) generate right pointer accessing expressions based on modes.
%
A-normal form conversion is straightforward: compound expressions are managed by storing results of subexpressions into temporary variables,
as in the following example.

{\vspace*{-0.5em}
{\small
\begin{center}
$
\begin{array}{l}
$\code{let y=(x+1)+(6+1)}$
\;
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\;
\begin{array}{l}
$\code{let a=x+1;}$\\
$\code{let b=6+1;}$\\
$\code{let y=a+b}$\\
\end{array}
\end{array}
$
\end{center}
}
}

This simplifies the management of effects from subexpressions. The
next two steps of compilation are more interesting.
We state them based on different \lang operations.

\begin{figure}[t!]
  \begin{small}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
int deref_array(n : int,
     p :  $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\cmode}$,
     q : $\color{green!40!black}\tntarrayptr{0}{n}{\tint}{\tmode}$) {
  /* $\color{purple!40!black}\rho$(p) = p_lo,p_hi,p_m */
  /* $\color{purple!40!black}\rho$(q) = q_lo,q_hi,q_m */
    * p;
    * q = 1;
}
...
/* p0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\cmode}$ */
/* q0 : $\color{purple!40!black}\tntarrayptr{0}{5}{\tint}{\tmode}$ */
deref_array(5, p0, q0);
    \end{lstlisting}
\begin{frame}

\tikz\draw[-Latex,line width=2pt,color=orange] (0,0) -- (1,0);

\end{frame}
\begin{lstlisting}[mathescape,xleftmargin=4 mm]
deref_array(int n, int* p, int * q) {
  //m is the current context mode
  let p_lo = 0; let p_hi = n; 
  let q_lo = 0; let q_hi = n; 
  /* runtime checks */
  assert(p_lo <= 0 && 0 <= p_hi);
  assert(p != 0);
  *(mode(p) $\wedge$ m,p);
  verify(q, not_null(m, q_lo, q_hi) 
             && q_lo <= 0 && 0 <= q_hi);
  *(mode(q) $\wedge$ m,q)=1;
}
...
deref_array(5, p0, q0);
    \end{lstlisting}
\end{small}
    \caption{Compilation Example for Dependent Functions}
\label{fig:compilationexample1}
\end{figure}

% \review{Fig 9: if this is actual C code, then your null-check at line 6 will be
%   eliminated by the compiler. At line 3, you performed a pointer addition, which
%   is only defined when `p` is non-null. So, either `p` is non-null, and the
%   NULL-check can be eliminated; or, `p` is NULL, but line 3 was undefined
%   behavior, meaning the compiler is allowed to do anything, notably eliminate
%   the NULL-check. This is where I am super confused, and either:
%   - CoreC is not really the C language, and has different semantics...? but is
%     this well-defined in the context of LLVM?
%   - there is a problem that was not caught by the PLT-Redex-based testing.
% \yiyun{We have clarified at the start of IV that CoreC is an untyped
%   variant of CoreChkC, and does not aim to represent C per se, or LLVM
%   IR. We aimed to avoid confusion by rewriting the examples in a way that is more closely
%     related to the syntax presented in Fig 3. Pointer arithmetic between 0 and a non-zero
%     index is always valid because CoreC there is technically only
%     integer arithmetic.}}

\myparagraph{Pointer Accesses and Modes}
%
In every declaration \lc{(or the beginning of a function body) why?} of a pointer,
if the poniter is an (NT-)array,
we first allocate two \emph{shadow variables}
to track the lower and upper bounds which are potentially changed for pointer arithmetic and NT-array bound widening.
Each $\cmode$-mode NT-array pointer variable is associated with its type information in a store.
Additionally, we place bounds and null-pointer checks, such as the line 6 and 7 in \Cref{fig:compilationexample1}.
In addition, in the formalism, before every use of a tainted pointer (\Cref{fig:compilationexample1} line 9 and 10), 
there is an inserted verification step similar to \Cref{fig:const-type},
which checks if a pointer is well defined in the heap (\code{not_null}) and the spatial safety.
Predicate \code{not_null} checks that every element in the pointer's range (\code{p_lo} and \code{p_hi}) is well defined in the heap.  
The modes in compiled deference (\code{*(mode(p) }$\wedge$\code{ m,p)})
 and assignment (\code{*(mode(q) }$\wedge$\code{ m,q)=1}) operations 
are computed based on the meet 
operation ($\wedge$) of the pointer mode (e.g. \code{mode(p)}) and the current context mode (\code{m}).

\myparagraph{Checked and Unchecked Blocks}
%
In the \systemname implementation,
$\euncheckedtext$ and $\echeckedtext$ blocks 
are compiled as context switching functions provided by the sandbox mechanism.
$\eunchecked{\overline{x}}{e}$ is compiled to 
$\texttt{sandbox\_call}(\overline{x},e)$, where we call the sandbox 
to execute expression $e$ with the arguments $\overline{x}$.
$\echecked{\overline{x}}{e}$ is compiled to 
$\texttt{callback}(\overline{x},e)$, where we perform 
a \texttt{callback} to a checked block code $e$ inside a sandbox.
In \systemname, we adopt an aggressive execution scheme that
directly learns pointer addresses from compiled assembly to make the $\texttt{callback}$ happen.
In the formalism, we rely on the type system to 
guarantee the context switching without creating the extra function calls for simplicity.

%Fig.~\ref{fig:compilationexample} shows how an invocation of
%\code{strlen} on a null-terminated string is compiled into C
%code. Each dereference of a checked pointer requires a null check
%(See \textsc{S-DefNull} in Fig.~\ref{fig:semantics}), which the
%compiler makes explicit: Line~$3$ of the generated code has the null
%check on pointer \code{p} due to the \code{strlen},
%  and a similar check happens
%  at line~$8$ due to the pointer arithmetic on \code{p}.
%Dereferences also require bounds checks: line~$2$ checks \code{p} is
%in bounds before computing \code{strlen(p)}, while line~$10$ does
%likewise before computing \code{*(p+1)}.

\myparagraph{Function Pointers and Calls}
%
Function pointers are managed similarly to normal pointers,
but we insert checks to check if the pointer address is not null in 
the function store instead of heap, and whether or not the type is correctly represented, 
for both $\cmode$ and $\tmode$ mode pointers 
\footnote{$\cmode$-mode pointers are checked once in the beginning and $\tmode$-mode pointers are checked every time when use}.
For example, in compiling the \code{stringsort} function in \Cref{fig:checkedc-example-1},
we place a check \code{verify_fun(cmp, not_null(c, p_lo, p_hi) && type_match)},
and we place a similar check before \Cref{fig:checkedc-example-3} line 7 to check the tainted \code{cmp} when it is used. 
The compilation of function calls (compiling to $\elcall{m}{e}{\overline{e}}$) 
is similar to the manipulation of pointer access operations in \Cref{fig:flagtable}.

For compiling dependent function calls,
\Cref{fig:compilationexample1} provides a hint.
Notice that the bounds for the array pointer \code{p} are not passed as
arguments. Instead, they are initialized according to \code{p}'s
type---see line~4 of the original \lang program at the top of the figure.
Line~$3$ of the generated code
sets the lower bound  to \code{0} and the
upper bound to \code{n}.

\subsection{Metatheory}
\label{sec:meta}

% \review{ 
% My interpretation of section IV.C is that the authors have a pen-and-paper
%   proof of Theorem 4, and that random testing using the Redex models has been
%   used to gain confidence in such a proof. Is this correct?}
% \liyi{Yes, this is correct.}
% \mwh{No, not correct: We have no pen-and-paper proof, just the
%   model. Clarify here}
% showing that
% extensive random testing fails to falsify the bisimilarity
% property. \mwh{Better way to state the previous?}

We formalize both the compilation procedure and the simulation
theorem in the PLT Redex model we developed for \lang (see Sec.~\ref{sec:syntax}),
and then attempt to falsify it via Redex's support for random
testing. Redex allows us
  to specify compilation as logical rules (an extension
  of typing), but then execute it algorithmically to
  automatically test whether simulation holds. This process revealed
  several bugs in compilation and the theorem statement.
%
  % us gain confidence that our original pen and paper proof of
  % simulation remains true with the addition of variable bounds. }
We ultimately plan to prove simulation in the Coq model.

%Turning to the simulation theorem: We first introduce notation
%used to specify the theorem.
We use the notation $\gg$ to
indicate the \emph{erasure} of stack and heap---the rhs is the same as
the lhs but with type annotations removed:
\begin{equation*}
  \begin{split}
    \heap  \gg & \dot \heap \\
    \varphi \gg & \dot \varphi
  \end{split}
\end{equation*}
In addition, when $\Gamma;\emptyset\vdash
\varphi$ and $\varphi$ is well-formed, we write $(\varphi,\heap,e) \gg_m (\dot \varphi, \dot \heap,
\dot e)$ to denote $\varphi \gg \dot \varphi$, $\heap \gg \dot \heap$
and $\Gamma;\Theta;\emptyset \vdash_m e \gg \dot e : \tau$ for some $\tau$ respectively. $\Gamma$ is omitted from the notation since the well-formedness of $\varphi$ and its consistency with respect to $\Gamma$ imply that $e$ must be closed under $\varphi$, allowing us to recover $\Gamma$ from $\varphi$.
Finally, we use $\xrightarrow{\cdot}^*$ to denote the transitive closure of the
reduction relation of $\elang$. Unlike the $\lang$, the semantics of
$\elang$ does not distinguish checked and unchecked regions.

Fig.~\ref{fig:checkedc-simulation-ref} gives an overview of 
the simulation theorem.\footnote{We ellide the  possibility of $\dot e_1$ evaluating to $\ebounds$ or $\enull$ in the diagram for readability.} The simulation theorem is specified in a way
that is similar to the one by~\citet{merigoux2021catala}.

An ordinary simulation property would
replace the middle and bottom parts of the figure with the
following: \[(\dot \varphi_0, \dot \heap_0, \dot e_0) 
  \xrightarrow{\cdot}^* (\dot \varphi_1, \dot \heap_1, \dot e_1)\]
Instead, we relate two erased configurations using the relation $\sim$,
which only requires that the two configurations will eventually reduce
to the same state.

\ignore{
 We formulate our simulation theorem differently
because the standard simulation theorem imposes a very strong
syntactic restriction to the compilation strategy. Very often, $(\dot
\varphi_0, \dot \heap_0, \dot e_0)$ reduces to a term that is
semantically equivalent to $(\dot \varphi_1, \dot \heap_1, \dot e_1)$,
but we are unable to syntactically equate the two configurations due
to the extra binders generated for dynamic checks and ANF
transformation. In earlier versions of the Redex model, we attempted
to change the compilation rules so the configurations could match
syntactically. However, the approach scaled poorly as we added
additional rules. 
This slight relaxation on the equivalence relation
between target configurations allows us to specify compilation more
naturally without having to worry about syntactic constraints.
}

% The two theorems are translation preservation and simulation. We donate $\xrightarrow{c}$ as the transition semantics of CLight.
\begin{thm}\label{simulation-thm}[Simulation ($\sim$)]
For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
if $\heap_0 \vdash \varphi_0$, $(\varphi_0,\heap_0,e_0)\gg_c (\dot \varphi_0,\dot \heap_0, \dot e_0)$,
and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
\rightarrow_c (\varphi_1, \heap_1, r_1)$, then the following facts hold:

\begin{itemize}

\item if there exists $e_1$ such that $r=e_1$ and $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
$(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\varphi,\dot \heap,\dot e)$ and $(\dot
\varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
\dot \heap,\dot e)$.

\item if $r_1 = \ebounds$ or $\enull$, then we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
\dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
\varphi_1$, $\heap_1 \gg \dot \heap_1$.

\end{itemize}
\end{thm}


% when $r_1 = e_1$ for
% some $e_1$ and
% $(\varphi_1, \heap_1, e_1) \gg (\dot \varphi_1, \dot \heap_1, \dot e_1)$, then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.

% \begin{thm}[Simulation ($\sim$)]\label{simulation-thm}
% For \lang expressions $e_0$, stacks $\varphi_0$, $\varphi_1$, and heap snapshots $\heap_0$, $\heap_1$, 
% if $\emptyset;\emptyset;\emptyset \vdash_\cmode e_0 \gg \dot e_0 :\tau_0$,
% and if there exists some $r_1$ such that $(\varphi_0, \heap_0, e_0)
% \rightarrow_\cmode (\varphi_1, \heap_1, r_1)$, when $r_1 = e_1$ for
% some $e_1$ and
% $\emptyset;\emptyset;\emptyset \vdash_\cmode e_1 \gg \dot e_1 :\tau_1$ where $\tau_1 \sqsubseteq \tau_0$
% , then
% there exists some $\dot \varphi$,$\dot \heap$, $\dot e$, such that
% $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \varphi,\dot \heap,\dot e)$ and $(\dot
% \varphi_1,\dot \heap_1,\dot e_1) \xrightarrow{\cdot}^* (\dot \varphi,
% \dot \heap,\dot e)$. When $r_1 = \ebounds$ or $\enull$, we have $(\dot \varphi_0, \dot \heap_0,\dot e_0) \xrightarrow{\cdot}^* (\dot
% \dot \varphi_1,\dot \heap_1, r_1)$ where $\varphi_1 \gg \dot
% \varphi_1$, $\heap_1 \gg \dot \heap_1$.
% \end{thm}

As our random generator never generates
$\euncheckedtext$ expressions (whose behavior could be undefined), we can only test a the simulation theorem 
as it relates to checked code. This limitation makes it
unnecessary to state the other direction of the simulation theorem
where $e_0$ is stuck, because Theorem~\ref{thm:progress} guarantees
that $e_0$ will never enter a stuck state if it is well-typed in
checked mode.

The current version of the Redex model has been tested against $23000$
expressions with depth less than $11$. Each expression can
reduce multiple steps, and we test simulation between every two
adjacent steps to cover a wider range of programs, particularly the
ones that have a non-empty heap.

\begin{figure}[t]
{\small
\[
\begin{array}{c}
\begin{tikzpicture}[
            > = stealth, % arrow head style
            shorten > = 1pt, % don't touch arrow head to node
            auto,
            node distance = 3cm
        ]

\begin{scope}[every node/.style={draw}]
    \node (A) at (0,1.5) {$\varphi_0,\heap_0, e_0$};
    \node (B) at (4,1.5) {$\varphi_1, \heap_1 ,e_1$};
    \node (C) at (0,0) {$\dot \varphi_0, \dot \heap_0 ,\dot e_0$};
    \node (D) at (4,0) {$\dot \varphi_1, \dot \heap_1, \dot e_1$};
    \node (E) at (2,-1.5) {$\dot \varphi,\dot \heap ,\dot e$};
\end{scope}
\begin{scope}[every edge/.style={draw=black}]

    \path [->] (A) edge node {$\longrightarrow_\cmode$} (B);
    \path [<->] (A) edge node {$\gg$} (C);
    \path [<->] (B) edge node {$\gg$} (D);
    \path [dashed,<->] (C) edge node {$\sim$} (D);
    \path [dashed,->] (C) edge node {$\xrightarrow{\cdot}^*$} (E);
    \path [dashed,->] (D) edge node[above] {$\xrightarrow{\cdot}^*$} (E);
\end{scope}

\end{tikzpicture}
\end{array}
\]
}
\caption{Simulation between \lang and \elang }
\label{fig:checkedc-simulation-ref}
\end{figure}

% \ignore
% {The two theorems can be best understood by the diagram in Fig.~\ref{fig:checkedc-simulation-ref}.
% % The diagram also indicates the simulation relation ($\sim$) among our formalization of the \checkedc Type System ($\vdash$), Semantics ($\longrightarrow$), Compilation ($\gg$), as well as the compiled C semantics ($\xrightarrow{c}$).
% On the top of the relation, we have type soundness theorem (Sec.~\ref{sec:theorem}) stating that every type-checked \lang program $e_0$ can transition via \lang semantics into a type-checked program $e_1$ by taking a checked step.
% The second line refers to that both $e_0$ and $e_1$ can be compiled to a C program $\dot e_0$ and $\dot e_1$, and such compilations always exist. The bottom two rewrites ($\xrightarrow{c}$) state that both $\dot e_0$ and $\dot e_1$ are transitioned finally to the same state $\dot e$. 

%  The simulation states that given any well-typed \lang expression $e_0$, its compiled C program as $\dot e_0$, if $e_0$ can transition one step to $e_1$, and there must exist the compiled C program $\dot e_1$, such that both $\dot e_0$ and $\dot e_1$ transition to the same final state $\dot e$ in C. The simulation relation $\sim$ is built on top of the "triangle" structure in Fig.~\ref{fig:checkedc-simulation-ref} stating that every \lang expression and its one step transitioned expression might be compiled to two different C program, but the C semantics always evaluates them to the same place.
% The translation preservation theorem is a support for the simulation theorem stating that for any well-typed \lang expression $e_0$,  its one step \checkedc transition expression $e_1$ must exist a compiled C program through the compilation process ($\gg$).

% A corollary of the simulation theorem states that for any \lang expression $e_0$ and its one step \lang transition expression $e_1$, such that $e_0 \longrightarrow e_1$, if $e_0$ contains a fault due to a unchecked block (the blame theorem tells us that if there is a fault in a \lang code, it must come from a unchecked block), and its compiled code $\dot e_0$ is evaluated to a fault state, the compiled code $\dot e_1$ for $e_1$ is always evaluated to the same fault state in C.
% This is an important property for a \lang compiler to preserve because any problematic program will be captured by running it long enough, so that enough random testing cases are able to capture all the bugs.
% In our formalization implementation, the type soundness theorem is proved through Coq, while the simulation theorem is validated through intensive random test case generation. We generate tens of thousands cases and cover all possible corner cases of the compilation. 
% }
% \ignore{
% \begin{figure*}[t]
%   \begin{subfigure}[b]{1\textwidth}
%     \begin{lstlisting}
% int deref_array (size_t n, nt_array_ptr<int> p : bounds(p, p + n)) {
%   return *p;
% }

% ...
% /* nt_array_ptr<int> p0 : bounds(p0, p0 + 5) */
% deref_array(5, p0);
%     \end{lstlisting}
%     \label{fig:chkcexamplederef}
%     \caption{\texttt{deref\_array} in \checkedc}
%   \end{subfigure}

%   \begin{subfigure}[b]{1\textwidth}
% \begin{lstlisting}
% int deref_array(size_t n, int *p) {
%   /* statically determine the definitions p_lo and p_hi */
%   int *p_lo = p;
%   int *p_hi = p + n;
%   /* runtime checks */
%   assert(p_lo <= p && p <= p_hi);
%   assert(p != NULL);
%   /* widening */
%   int p_derefed = *p;
%   if (p_derefed != '\0') {
%     if (p_hi == p) {
%       ++p_hi;
%     }
%   }
%   return p_derefed;
% }
% ...
% /* int *p0 */
% /* rho(p0) = p_lo, p_hi */
% deref_array(5, p0);
% \end{lstlisting}
% \caption{\texttt{deref\_array} in C}
%     \label{fig:cexamplederef}
%   \end{subfigure}

%   \begin{subfigure}[b]{\textwidth}
%     \begin{lstlisting}
% /* nt_array_ptr<int> p : bounds(e0, e1) */
% /* rho(p) = p_lo, p_hi */
% size_t x = strlen(p);
% if(x >= 1) {
%   *(p+1);
% }
%     \end{lstlisting}
%     \caption{\texttt{strlen} in \checkedc}
%     \label{fig:chkcexamplestrlen}
%   \end{subfigure}

%   \begin{subfigure}[b]{\textwidth}
%     \begin{lstlisting}
% /* nt_array_ptr<int> p : bounds(e0, e1) */
% /* rho(p) = p_lo, p_hi */
% /* runtime checks omitted */
% ...
% size_t x = strlen(p);
% int *p_hi_new = p + x;
% p_hi = max(p_hi, p_hi_new);
% if (x >= 1) {
%   /* null check for pointer arithmetic */
%   assert(p != NULL);
%   int *p_1 = p + 1;
%   /* null check for dereference */
%   assert(p_1 != NULL);
%   /* bounds check for dereference */
%   /* note how we are using p_hi rather than p + x */
%   assert(p_lo <= p_1 && p_1 <= p_hi);
%   ...
% }
%     \end{lstlisting}
%     \caption{\texttt{strlen} in C}
%     \label{fig:cexamplestrlen}
%   \end{subfigure}
% \caption{Compilation of null-terminated string dereference}
% \label{fig:compilationexample}
% \end{figure*}
% }

% \ignore{
% Fig.~\ref{fig:compilationexample} gives an example of how we compile the
% dereference of null-terminated strings by inserting explicit checks and
% bounds widening code. For readability, we present the example in real
% \checkedc and C syntax. At
% line 3-4 of Fig.~\ref{fig:cexamplederef}, we see how the upper and lower
% bounds are defined in terms of the arguments \code{p} and \code{n}
% according to the bounds annotations obtained during typechecking. This
% avoids the need to pass in bounds as extra arguments, thus maintaining
% compatibility with C code. When we call \code{deref_array} on
% \code{p0}, a string with size 5 (excluding the null character), there
% is no need to pass the bounds \code{p_lo} and \code{p_hi} stored on
% the stack. It is possible that \code{p_hi} has been widened
% before we reach line 21. The programmer will have to perform a
% \code{dynamic_bounds_cast} to recover the more precise bounds information. The if statement from line 10 to line 14
% attempts to widen the bound when the dereferenced result is not
% null. \todo[inline]{Show a full-fledged C program?} The widened upper bound will be available within the scope of
% \code{deref_array}, in contrast to the T-IfNT rule, which only
% remembers the widened bounds within the scope of the then branch of
% the if statement. 


% \todo[inline]{Widening can happen at every
%   dereference at runtime. Is that ok?} Fig.~\ref{fig:cexamplestrlen}
% shows how an invocation of \code{strlen} on null-termianted strings
% is compiled into C code. We perform the same runtime checks that
% happen during dereferencing. The widening code at line
% 8 updates \code{p}'s upper bound only if the result of
% \code{strlen} is larger than the value of the upper bound stored on
% the stack. This is another scenario where the runtime can be more precise
% than the statically determined bounds information.



% }

